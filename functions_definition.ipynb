{
 "metadata": {
  "name": "",
  "signature": "sha256:ce7942154427550f3205b2b6cc3610e96250590eb3235f33f246f46c9d1693cf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Design of Primers for Functional Genes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Author: Teotonio Soares de Carvalho"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Please contact me at teotonio@msu.edu if you have any suggestions or questions."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Setup for Parallel Processing"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Requires an IPcluster running with at least one cluster.\n",
      "# An error will be raised if no cluster is found.\n",
      "try:\n",
      "    import os\n",
      "    from IPython.parallel import Client\n",
      "    rc = Client()\n",
      "    cwd = os.getcwd() \n",
      "    # To make sure that the works are operating on the same work dir.\n",
      "    rc[:].execute('import os;os.chdir(%s)'%cwd)\n",
      "    dview = rc[:]\n",
      "    dview.block = False\n",
      "    lview = rc.load_balanced_view()\n",
      "except:\n",
      "    raise Exception(\"Please, start an IPython cluster to proceed.\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def wait_on(ar, verbose = True):\n",
      "    \"\"\"\n",
      "    Tracks the progress of a task running on a IPcluster. Downloaded from the internet.\n",
      "    \"\"\"\n",
      "    from datetime import datetime\n",
      "    N = len(ar.msg_ids)\n",
      "    rc = ar._client\n",
      "    submitted = rc.metadata[ar.msg_ids[0]]['submitted']\n",
      "    while not ar.ready():\n",
      "        ar.wait(1)\n",
      "        msgs = [msg_id not in rc.outstanding for msg_id in ar.msg_ids]\n",
      "        progress = sum(msgs)\n",
      "        dt = (datetime.now()-submitted).total_seconds()\n",
      "        if verbose:\n",
      "            clear_output()\n",
      "            print \"%3i/%3i tasks finished after %4i s\" % (progress, N, dt),\n",
      "            sys.stdout.flush()\n",
      "    if verbose:\n",
      "        print\n",
      "        print \"done\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Import modules"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time, os, shutil, math, sys, copy, random, time \n",
      "import regex    # Make sure you have installed this module. Use: pip install regex\n",
      "import pandas   # Make sure you have installed this module as well and its dependencies. \n",
      "import numpy as np  # If you installed pandas successfully, this package is already installed\n",
      "from selenium import webdriver # Selenium and webdriver are installed separately\n",
      "from selenium.webdriver.support.select import Select\n",
      "from Bio.Seq import Seq  # This also have to be installed: pip install Biopython\n",
      "from Bio import SeqIO, AlignIO\n",
      "from Bio.SeqRecord import SeqRecord\n",
      "from IPython.core.display import clear_output\n",
      "from itertools import combinations, permutations, izip, product, izip_longest\n",
      "import datetime as dt\n",
      "from functools import partial\n",
      "import primer3\n",
      "from IPython.core.display import clear_output\n",
      "import commands\n",
      "with dview.sync_imports():\n",
      "    from functools import partial\n",
      "    import numpy\n",
      "    import primer3\n",
      "    from Bio import SeqIO, Seq\n",
      "    from string import Template\n",
      "    import regex\n",
      "    import os\n",
      "    from Bio.Seq import Seq\n",
      "    import pandas\n",
      "    from itertools import combinations, izip, product, permutations"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "importing partial from functools on engine(s)\n",
        "importing numpy on engine(s)\n",
        "importing primer3 on engine(s)\n",
        "importing SeqIO,Seq from Bio on engine(s)\n",
        "importing Template from string on engine(s)\n",
        "importing regex on engine(s)\n",
        "importing os on engine(s)\n",
        "importing Seq from Bio.Seq on engine(s)\n",
        "importing pandas on engine(s)\n",
        "importing combinations,izip,product,permutations from itertools on engine(s)\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Download Archaea amoA Sequences from Fungene"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_driver(gene_url):\n",
      "    \"\"\"\n",
      "    Loads the fungene web page in chrome and assign it to the global variable \"driver\".\n",
      "    I could not use the module ghost, which is faster, because the repository and the \n",
      "    analysis page in fungene are in separate tabs.\n",
      "    Requires webdriver, chrome, and selenium.\n",
      "    \"\"\"\n",
      "    global driver\n",
      "    if \"driver\" in dir(): \n",
      "        driver.quit()\n",
      "    driver = webdriver.Chrome()\n",
      "    driver.get(gene_url);\n",
      "    driver.find_element_by_partial_link_text(\"Display Options\").click()\n",
      "    seq = Select(driver.find_element_by_id(\"seqsPerPage\"))\n",
      "    seq.select_by_value(\"2000\")\n",
      "    driver.find_element_by_id(\"displayCmd\").submit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_by_score(score):\n",
      "    \"\"\"\n",
      "    Selects the minimum score as 400 in the repository page.\n",
      "    \"\"\"\n",
      "    form = driver.find_element_by_name(\"hmmId\")\n",
      "    driver.find_element_by_link_text(\"Show/Hide filter options\").click()\n",
      "    min_score = driver.find_element_by_name(\"min_bits\")\n",
      "    min_score.send_keys(str(score))\n",
      "    form.submit()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_last_page():\n",
      "    \"\"\"\n",
      "    Detects the total number of pages using regular expression.\n",
      "    \"\"\"\n",
      "    pages = regex.findall(r\"page=([0-9]*)\", driver.page_source)\n",
      "    if not pages:\n",
      "        # Added to correct an error when there is only one page\n",
      "        last_page = 1\n",
      "    else:\n",
      "        last_page = max([int(page) for page in pages])\n",
      "    return last_page"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def load_page_repository(page_number, gene_url):\n",
      "    \"\"\"\n",
      "    Loads a page, specified by \"page_number\", for the archaeal amoA\n",
      "    from the fungene repository and select all the sequences in it.\n",
      "    Arguments:\n",
      "        - page_number: an integer specifying the desired page.\n",
      "    \"\"\"\n",
      "    url = gene_url + \"&page=%d\"%page_number\n",
      "    driver.get(url)\n",
      "    driver.find_element_by_link_text(\"Select Entire Page\").click()\n",
      "    get_nucl2prot_accession()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_fungene(download_path):\n",
      "    \"\"\"\n",
      "    Removes files previously download from fungene in the default\n",
      "    download directory used by chrome.\n",
      "    Arguments:\n",
      "        - download_path: a string indicating  the default download folder.\n",
      "    \"\"\"\n",
      "    for file in os.listdir(download_path):\n",
      "        if regex.match(r\"fungene.*?aligned_(nucleotide)*(protein)*_seqs\", file):\n",
      "            os.remove(download_path + file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_nucl2prot_accession():\n",
      "    \"\"\"\n",
      "    Extracts the respective acc for protein and nucleotide for each sequence\n",
      "    and saves the results in a file \"./data/nucleotide2protein\" where each\n",
      "    line is as follow:\n",
      "    acc for protein | acc for nucleotide\n",
      "    \"\"\"\n",
      "    reg_exp = regex.compile(r\"gpprotdata.jsp\\?seqAccno=([0-9A-Z]+).+?\"\n",
      "                             \"gbnucdata.jsp\\?seqAccno=([0-9A-Z]+)\", \n",
      "                             regex.DOTALL|regex.MULTILINE|regex.VERBOSE)\n",
      "    accession = reg_exp.findall(driver.page_source)\n",
      "    accession = \"\\n\".join([\"|\".join(pair) for pair in accession])\n",
      "    with open(\"./data/download/nucleotide2protein\", \"a\") as handle:\n",
      "        handle.write(accession + \"\\n\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def download_hmm(default_download_path):\n",
      "    \"\"\"\n",
      "    Download the hmm for AOA and copy it to ./data/\n",
      "    Arguments:\n",
      "        - download path: string indicating the default download folder\n",
      "    \"\"\"\n",
      "    for file in os.listdir(default_download_path):\n",
      "        if \".hmm\" in file:\n",
      "            os.remove(default_download_path + file)\n",
      "    driver.find_element_by_link_text(\"(download HMM)\").click()\n",
      "    time.sleep(1)\n",
      "    while True:\n",
      "        files = os.listdir(default_download_path)\n",
      "        file = [file for file in files if \".hmm\" in file]\n",
      "        if file:\n",
      "            file = file[0]\n",
      "            break\n",
      "    time.sleep(2)\n",
      "    shutil.copy(default_download_path + file, \"./data/download/\" + file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def switch_to_analysis_window():\n",
      "    \"\"\"\n",
      "    Switchs from repository windows to the analysis windows.\n",
      "    Fails if the analysis windows is not already open.\n",
      "    \"\"\"\n",
      "    driver.switch_to.window(driver.window_handles[1])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def switch_to_repository_window():\n",
      "    \"\"\"\n",
      "    Switches back to the repository windows.\n",
      "    \"\"\"\n",
      "    driver.switch_to.window(driver.window_handles[0])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def deselect_all_sequences():\n",
      "    \"\"\"\n",
      "    Deselect sequences already downloaded in the repository page.\n",
      "    \"\"\"\n",
      "    try:\n",
      "        driver.find_element_by_link_text(\"Deselect All Sequences\").click()\n",
      "    except:\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def download_sequences(default_download_path, count):\n",
      "    \"\"\"\n",
      "    Initiate the \"Begin Analysis\" link for the fungene repository to\n",
      "    download the sequences. Downloads the unaligned nucleotides.\n",
      "    \"\"\"\n",
      "    driver.find_element_by_link_text(\"Begin Analysis\").click()\n",
      "    switch_to_analysis_window()\n",
      "    for seq_type in [\"Nucleotide\", \"Protein\"]:\n",
      "        driver.find_element_by_id(\"download_%s_seqs\"%seq_type).click()\n",
      "        # Uncheck the aligned option if checked. \n",
      "        aligned_option = driver.find_element_by_id(\"aligned1\")\n",
      "        if aligned_option.is_selected(): \n",
      "            aligned_option.click()\n",
      "        # Download the sequences\n",
      "        driver.find_element_by_name(\"download\").click()\n",
      "        # If the internet is slow it is better to increase this time\n",
      "        time.sleep(2) \n",
      "        move_file_to_data(default_download_path, count, seq_type)\n",
      "    switch_to_repository_window()\n",
      "    deselect_all_sequences()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def move_file_to_data(default_download_path, count, seq_type):\n",
      "    exp = r\"fungene.*?aligned_%s_seqs\"%seq_type.lower()\n",
      "    files = os.listdir(default_download_path)\n",
      "    file = [file for file in files if regex.match(exp, file)][0]\n",
      "    with open(default_download_path + file, \"r\") as handle:\n",
      "        fasta = handle.read()\n",
      "    os.remove(default_download_path + file)\n",
      "    file_name = \"./data/download/%s_%d\"%(seq_type.lower(), count)\n",
      "    with open(file_name, \"w\") as handle:\n",
      "        handle.write(fasta)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gather_all_fasta(protein = False):\n",
      "    \"\"\"\n",
      "    Gather all download nucleotide unaligned sequences in one fasta\n",
      "    file at \"./data/arch_amoa_all.fasta\"\n",
      "    Arguments:\n",
      "        - download path: string indicating the default download folder\n",
      "    \"\"\"\n",
      "    if protein:\n",
      "        seq_type = \"protein\"\n",
      "    else:\n",
      "        seq_type = \"nucleotide\"\n",
      "    with open(\"./data/download/all_%s\"%seq_type, \"w\") as handle:\n",
      "        for file in os.listdir(\"./data/download/\"):\n",
      "            if regex.match(r\"%s_[0-9]+\"%seq_type, file):\n",
      "                with open(\"./data/download/\" + file, \"r\") as handle_split:\n",
      "                    handle.write(handle_split.read())\n",
      "                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main_download(hmm_id, default_download_path, score):\n",
      "    \"\"\"\n",
      "    Downloads the proteins, nucleotides, hmm file and the correspondence between acession numbers from proteins\n",
      "    and nucleotides.\n",
      "    Arguments:\n",
      "        -hmm_id: an integer giving the fungene hmm_id for a particular gene. You can get it by clicking in the gene link\n",
      "                 in the fungene database. In the url that appears in the address bar you will see the hmm_id.\n",
      "        -default_download_path: string giving the default path for download for chrome.\n",
      "        -score: an integer giving the minimum hmm score for the sequences. Be careful with this parameter as it is very\n",
      "                gene dependent. That is why there is no default value.\n",
      "    This function uses chromedriver to automate the download. While the browser is working to download the files you should \n",
      "    not interact with it or unexpected results may arise. It usually takes about 2 minutes to complete the download on a \n",
      "    good network. If you notice that any of the download fails, remove the data folder, restart the notebook and run again.\n",
      "    \"\"\"\n",
      "    # Remove previous files to avoid errors\n",
      "    if os.path.isdir(\"./data/\"):\n",
      "        shutil.rmtree(\"./data/\")\n",
      "    os.makedirs(\"./data/\")\n",
      "    os.makedirs(\"./data/download/\")\n",
      "    gene_url = \"http://fungene.cme.msu.edu/hmm_details.spr?hmm_id=%d\"%hmm_id\n",
      "    remove_fungene(default_download_path)\n",
      "    load_driver(gene_url)\n",
      "    download_hmm(default_download_path)\n",
      "    filter_by_score(score)\n",
      "    last_page = find_last_page()\n",
      "    deselect_all_sequences()\n",
      "    count = 1\n",
      "    for page_number in range(1, last_page + 1):\n",
      "        load_page_repository(page_number, gene_url)\n",
      "        # Download the sequences when repository page is multiple \n",
      "        # of 5 or the last page\n",
      "        if not page_number % 5 or page_number == last_page:\n",
      "            download_sequences(default_download_path, count)\n",
      "            time.sleep(1)\n",
      "            count += 1\n",
      "    gather_all_fasta()\n",
      "    gather_all_fasta(protein = True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Align Protein"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def read_fasta(filename):\n",
      "    \"\"\"\n",
      "    Reads a fasta file and return it as a list of records.\n",
      "    Arguments:\n",
      "        - filename: string indicating the name of the file \n",
      "                    including the path.\n",
      "    Returns a list of records formatted by BioPython.\n",
      "    \"\"\"\n",
      "    with open(filename, \"rU\") as handle:\n",
      "        records = list(SeqIO.parse(handle, \"fasta\"))\n",
      "    return records\n",
      "\n",
      "def write_fasta(filename, records_list):\n",
      "    \"\"\"\n",
      "    Takes a list of records (BioPython) and writes it to filename.\n",
      "    \"\"\"\n",
      "    with open(filename, \"w\") as handle:\n",
      "        fasta_writer = SeqIO.FastaIO.FastaWriter(handle)\n",
      "        fasta_writer.write_file(records_list)\n",
      "\n",
      "def make_dict_records(fasta_file_name):\n",
      "    \"\"\"\n",
      "    Returns a dict of records where the keys are the accession\n",
      "    number and the values are the records (Biopython)\n",
      "    \"\"\"\n",
      "    records = read_fasta(fasta_file_name)\n",
      "    records_dict = {record.name:record for record in records}\n",
      "    return records_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def align_protein():\n",
      "    \"\"\"\n",
      "    Takes the unaligned proteins in ./data/download and align them using the \n",
      "    hmm profile.\n",
      "    \"\"\"\n",
      "    file = [file for file in os.listdir(\"./data/download/\") if \".hmm\" in file][0]\n",
      "    cmd = (\"hmmalign \" \n",
      "           \"./data/download/%s \"\n",
      "           \"./data/download/all_protein \"\n",
      "           \"> ./data/download/aligned_prot \")%file\n",
      "    print \"Aligning sequences\"\n",
      "    process = os.system(cmd)\n",
      "    if process:\n",
      "        print cmd\n",
      "        raise\n",
      "    print \"Reading Alignment\"\n",
      "    alignment = AlignIO.read(open(\"./data/download/aligned_prot\"), \"stockholm\")\n",
      "    print \"Writing Alignment\"\n",
      "    write_fasta(\"./data/download/aligned_prot\", alignment)\n",
      "    sys.stdout.flush()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Align Nucleotides Using Proteins"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_nuc2prot():\n",
      "    \"\"\"\n",
      "    Returns a dict of nucleotide accessions numbers as keys and \n",
      "    protein acession numbers as values.\n",
      "    \"\"\"\n",
      "    nuc2prot_acc = {}\n",
      "    with open(\"./data/download/nucleotide2protein\", \"r\") as handle:\n",
      "        line = handle.readline()\n",
      "        while line:\n",
      "            prot, nuc = line.split(\"|\")\n",
      "            nuc2prot_acc[nuc[:-1]] = prot\n",
      "            line = handle.readline()\n",
      "    return nuc2prot_acc"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def align_nucleotides():\n",
      "    \"\"\"\n",
      "    Takes the protein alignment positions and align the nucleotide codons\n",
      "    based on that.\n",
      "    Raises an error if the frame is not corrected.\n",
      "    \"\"\"\n",
      "    nucleotides = make_dict_records(\"./data/download/all_nucleotide\")\n",
      "    proteins = make_dict_records(\"./data/download/aligned_prot\")\n",
      "    nuc2prot_acc = get_nuc2prot()\n",
      "    aligned_nucleotides = []\n",
      "    for nuc_acc, nucleotide in nucleotides.iteritems():\n",
      "        protein = proteins[nuc2prot_acc[nuc_acc]]\n",
      "        prot_unaligned = regex.sub(r\"[-\\.]\", \"\", str(protein.seq))\n",
      "        prot_transl = str(nucleotide.seq.translate())\n",
      "        #Checking if frames are right\n",
      "        nuc_seq = str(nucleotide.seq)\n",
      "        protein_seq = str(protein.seq)\n",
      "        codons = [nuc_seq[i: i+3] for i in xrange(0, len(nuc_seq), 3)]\n",
      "        aligned_seq = []\n",
      "        for position in protein_seq:\n",
      "            if position == \".\" or position == \"-\":\n",
      "                aligned_seq += [\"---\"]\n",
      "            else:\n",
      "                if codons:  \n",
      "                    codon = codons.pop(0)\n",
      "                    aligned_seq += codon\n",
      "                else:\n",
      "                    aligned_seq += [\"---\"]\n",
      "        nucleotide.seq = Seq(\"\".join(aligned_seq))\n",
      "        aligned_nucleotides += [nucleotide]\n",
      "        if not prot_transl.upper()[1:-1] in prot_unaligned.upper():\n",
      "            print \"Incorrect frame!\"\n",
      "            raise\n",
      "    write_fasta(\"./data/download/aligned_nucleotides\", aligned_nucleotides)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 19
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Trimm Alignment"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def count_invalid_pos(file_name = None, records = None):\n",
      "    \"\"\"\n",
      "    Takes an aligment and counts the number of gaps \".\" or \"-\".\n",
      "    Arguments:\n",
      "        - file_name: if given, the function reads the fasta file.\n",
      "        - records: a list of records from an aligment (optional)\n",
      "    Only one of these two arguments must be provided.\n",
      "    \"\"\"\n",
      "    if not records:\n",
      "        records = read_fasta(file_name)\n",
      "    nseq = len(records)\n",
      "    seqs = [str(record.seq.upper()) for record in records]\n",
      "    align_pos = izip(*seqs)\n",
      "    count_pos = {}\n",
      "    for count, bases in enumerate(align_pos):\n",
      "        count_pos[count] = len(regex.findall(r\"[-\\.]\", \"\".join(bases)))\n",
      "    count = nseq - pandas.Series(count_pos)\n",
      "    return (count, nseq)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trimm_columns(records, prop_non_gap = .001, column_info_prop_discard = .3):\n",
      "    \"\"\"\n",
      "    Removes columns of the aligment in both ends where the proportion of\n",
      "    gaps is higher than 30%. It DOESN'T remove positions in the middle of the\n",
      "    aligment.\n",
      "    \"\"\"\n",
      "    count, nseq = count_invalid_pos(records = records)\n",
      "    drop_threshold = prop_non_gap * nseq\n",
      "    richer_pos = count[count / float(nseq) > column_info_prop_discard]\n",
      "    start = richer_pos.index[0]\n",
      "    end = richer_pos.index[-1]\n",
      "    count_richer_pos = count[start:end + 1]\n",
      "    positions_to_drop = list(count_richer_pos[count_richer_pos <= drop_threshold].index)\n",
      "    positions_to_keep = [i for i in range(start, end + 1)]\n",
      "    trimmed_records = []\n",
      "    for record in records:\n",
      "        seq = str(record.seq)\n",
      "        seq = \"\".join([seq[i] for i in positions_to_keep])\n",
      "        record.seq = Seq(seq)\n",
      "        trimmed_records += [record]\n",
      "    return trimmed_records"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def dealign_seq(in_file_name = \"./data/trimmed_align\",\n",
      "                out_file_name = \"./data/unaligned_trimmed\"):\n",
      "    \"\"\"\n",
      "    Removes gaps from sequences.\n",
      "    \"\"\"\n",
      "    records = read_fasta(in_file_name)\n",
      "    for record in records:\n",
      "        record.seq = Seq(regex.sub(r\"[\\.-]\", \"\", str(record.seq))).upper()\n",
      "        record.description = \"\"\n",
      "    write_fasta(out_file_name, records)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def trimm_records(max_prop_gap_ends = .1, column_info_prop_discard = .3):\n",
      "    \"\"\"\n",
      "    Removes positions of the alignment with less than a specified threshold of information (bases) and\n",
      "    discards sequences (after trimming the positions) with more a threshold of its length as gaps at any\n",
      "    of its ends.\n",
      "    Arguments:\n",
      "        -column_info_prop_discard: minimal allowed proportion of information (bases) in the columns at both ends\n",
      "                                   of the aligment. The purporse is to delete columns in both ends for which information\n",
      "                                   is not available for most sequences.\n",
      "        -max_prop_gap_ends: maximal proportion of the length a sequence (after trimming columns as above) that consists of\n",
      "                            continuous gaps in any of the ends of the sequence. The purpose is to remove sequences are too\n",
      "                            short and cannot be used to test the primers.\n",
      "    \"\"\"\n",
      "    records = read_fasta(\"./data/download/aligned_nucleotides\")\n",
      "    trimmed_columns = trimm_columns(records, column_info_prop_discard = column_info_prop_discard)\n",
      "    npos = len(trimmed_columns[0].seq)\n",
      "    max_gap_ends = max_prop_gap_ends * npos\n",
      "    records_to_drop = 0\n",
      "    records_to_keep = []\n",
      "    dropped = 0\n",
      "    for count, record in enumerate(trimmed_columns):\n",
      "        seq = str(record.seq)\n",
      "        gaps = regex.search(\"(?P<start>^[-\\.]*).+?(?P<end>[-\\.]*$)\", seq)\n",
      "        if (len(gaps[\"start\"]) >= max_gap_ends) or (len(gaps[\"end\"]) >= max_gap_ends):\n",
      "            dropped += 1\n",
      "            records_to_drop += 1\n",
      "        else:\n",
      "            records_to_keep += [record]\n",
      "    write_fasta(\"./data/trimmed_align\", records_to_keep)\n",
      "    print \"%d sequences had more gaps in their ends than the specified threshold and were deleted.\"%dropped"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Remove redundancy at 100% Similarity"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def cluster_sequences(input_file, output_file, similarity, word_size):\n",
      "    \"\"\"\n",
      "    Cluster all sequences and write two files:\n",
      "        - ./data/arch_amoa_repr.clstr with information for each cluster\n",
      "        - ./data/arch_amoa_repr a fasta file with representatives (not used)\n",
      "    \"\"\"\n",
      "    if os.path.isfile(output_file):\n",
      "        os.remove(output_file)\n",
      "    cmd = Template(\n",
      "     \"cd-hit-est -i $input_file \"   # Input File\n",
      "                \"-o $output_file \"  # Output File\n",
      "                \"-c $similarity \"   # Threshold similarity\n",
      "                \"-n $word_size \"    # Word size (see manual)\n",
      "                \"-T 0 \"             # Use all processors\n",
      "                \"-M 0\")             # Use all memory\n",
      "    cmd =  cmd.substitute(input_file = input_file,\n",
      "                          output_file = output_file,\n",
      "                          similarity = similarity,\n",
      "                          word_size = word_size)\n",
      "    print cmd\n",
      "    process = os.system(cmd)\n",
      "    if process:\n",
      "        print cmd\n",
      "        raise"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Create fasta file for each cluster"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This code was reused from previous versions. That is the reason for the fancy Arch_Group class"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_split_groups_out(file_name):\n",
      "    \"\"\"\n",
      "    Breaks the output of cd-hit and return it as a list of\n",
      "    strings were each element correspond to a group.\n",
      "    \"\"\"\n",
      "    with open(file_name, \"r\") as handle:\n",
      "        groups = handle.read()\n",
      "    groups_split = groups.split(\">Cluster\")[1:]\n",
      "    return groups_split"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class Arch_Group:\n",
      "    \"\"\"\n",
      "    Used to parse the results from cd-hist. An instance of Arch_Group is a group of\n",
      "    nucleotide sequences that are 97% similar. \n",
      "    The instance have the following properties:\n",
      "        - name: the name of the representative sequence\n",
      "        - representative(deprecated): same as above\n",
      "        - n_members: number of sequences in group\n",
      "        - members: a list of strings with members accession number\n",
      "        - nuc_members: list of integers giving the number of nuc for \n",
      "                       each member.\n",
      "        - n_invalid(deprecated): number of non ACTG characters in the representative seq\n",
      "        - seq(deprecated): the seq of the representative.\n",
      "    Note:\n",
      "        The representative is not used in the subsequent analysis. Instead of using\n",
      "        a representative sequence, I use the consensus in the next steps. But the \n",
      "        groups will be identified by the representative acc number.\n",
      "    \"\"\"\n",
      "    def __init__(self,group, dict_fasta):\n",
      "        self.get_representative(group)\n",
      "        self.name = self.representative\n",
      "        self.get_members(group)\n",
      "        self.get_nuc_numbers(group)\n",
      "        self.n_members = len(self.members)\n",
      "        self.seq = dict_fasta[self.name].seq\n",
      "        self.get_n_invalid_bases_representative()\n",
      "    \n",
      "    def __repr__(self):\n",
      "        return \"Group: %s, %d sequence(s)\"%(self.name, self.n_members)\n",
      "    \n",
      "    def get_representative(self, group):\n",
      "        repr_id = regex.findall(r\"\\>([A-Z|0-9]*?[_0-9]*)\\.\\.\\.\\ \\*\", group)[0]\n",
      "        self.representative = repr_id\n",
      "    \n",
      "    def get_members(self, group):\n",
      "        seqs_id = regex.findall(r\"\\>([A-Z|0-9]*?[_0-9]*)\\.\\.\\.\", group)\n",
      "        self.members = seqs_id\n",
      "            \n",
      "    def get_nuc_numbers(self, group):\n",
      "        nuc_numbers = regex.findall(r\"([0-9]{1,4})nt,\", group)\n",
      "        nuc_numbers = [int(num) for num in nuc_numbers]\n",
      "        self.nuc_numbers = nuc_numbers\n",
      "    \n",
      "    def get_sequences(self, records_dict):\n",
      "        self.sequences = [records_dict[id] for id in self.sequences_ids]\n",
      "        \n",
      "    def get_n_invalid_bases_representative(self):\n",
      "        n_invalid = len(regex.findall(r\"[^atgc]\", str(self.seq)))\n",
      "        self.n_invalid = n_invalid\n",
      "    \n",
      "    def get_consensus_record(self):\n",
      "        record = read_fasta(\"./data/consensus/%s.fasta\"%self.name)\n",
      "        self.consensus_record = record\n",
      "    def get_members_record_list(self):\n",
      "        records = read_fasta(\"./data/groups/%s.fasta\"%self.name)\n",
      "        self.members_record_list = records"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_dict_groups(group_file_name = \"./data/cluster_97.clstr\",\n",
      "                     fasta_file_name = \"./data/contigs_100\"):\n",
      "    \"\"\"\n",
      "    Parses the arch_amoa_repr.clstr and returns a dict\n",
      "    where the key is the name of the group and the values\n",
      "    are instances of the Arch_Group\n",
      "    \"\"\"\n",
      "    dict_fasta = make_dict_records(fasta_file_name)\n",
      "    split_groups_out = make_split_groups_out(group_file_name)\n",
      "    groups = [Arch_Group(group, dict_fasta) for group in split_groups_out]\n",
      "    groups = {group.name:group for group in groups}\n",
      "    return groups"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_fasta_for_groups(dict_groups, dict_fasta, path_name):\n",
      "    \"\"\"\n",
      "    Creates one fasta file for each groups containing the sequences\n",
      "    for that group.\n",
      "    Arguments:\n",
      "        - dict_groups: a dict of groups as returned by make_dict_groups \n",
      "        - dict_fasta: a dict of sequences\n",
      "        - path_name: the path where the fasta file for the grouped must be saved\n",
      "    \"\"\"\n",
      "    if os.path.isdir(path_name):\n",
      "        shutil.rmtree(path_name)\n",
      "    os.makedirs(path_name)\n",
      "    for group_name, group in dict_groups.iteritems():\n",
      "        records = [dict_fasta[member] for member in group.members]\n",
      "        write_fasta(path_name + group_name, records)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main_make_fasta_from_groups(group_name, fasta_file_name, path_name):\n",
      "    groups = make_dict_groups(group_name, fasta_file_name)\n",
      "    dict_fasta = make_dict_records(fasta_file_name)\n",
      "    make_fasta_for_groups(groups, dict_fasta, path_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 29
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Step 05. Make Consensus Sequences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_consensus(group_name, aligned_path, consensus_path, plurality):\n",
      "    \"\"\"\n",
      "    Find the consensus sequence for each aligned fasta file\n",
      "    in ./data/aligned/\n",
      "    Arguments:\n",
      "        - group_name: string indicating the name of the group\n",
      "        - aligned_path: string indicating the path where the aligned sequences are.\n",
      "        - consensus_path: string indicating where the consensus are.        \n",
      "        - plurality: the minimal proportion of agreement that must be at a given position for\n",
      "                     the consensus to receive the most frequent base at that position.\n",
      "\n",
      "    Writes the consensus sequences to ./data/consensus\n",
      "    \"\"\"\n",
      "    n_seq = len(read_fasta(aligned_path + group_name))\n",
      "    min_agreement = int(plurality * n_seq)\n",
      "    cmd = Template(\"cons \"                         # From emboss\n",
      "           \"$aligned_path$group_name \"      # Input fasta\n",
      "           \"$consensus_path$group_name \"    # Output fasta\n",
      "           \"-name $group_name \"  # The consensus sequence is named with the group name\n",
      "           \"-plurality $min_agreement \" # See description of the function above\n",
      "          )\n",
      "    cmd = cmd.substitute(aligned_path = aligned_path,\n",
      "                         consensus_path = consensus_path,\n",
      "                         group_name = group_name,\n",
      "                         min_agreement = min_agreement)\n",
      "    process = os.system(cmd)\n",
      "    if process:\n",
      "        raise RuntimeError('program {} failed!'.format(cmd))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def gather_all_consensus(consensus_path, consensus_name):\n",
      "    \"\"\"\n",
      "    Arguments:\n",
      "        Gather the individual consensus file in a unique file.\n",
      "        - consensus_path: the path where the consensus file are.\n",
      "        - consensus_name: the name of the resulting consensus file.\n",
      "    \"\"\"\n",
      "    records = \"\"\n",
      "    for file in os.listdir(consensus_path):\n",
      "        with open(consensus_path + file, \"r\") as handle:\n",
      "            records += handle.read()\n",
      "    with open(consensus_name, \"w\") as handle:\n",
      "        handle.write(records)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 31
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main_parallel_consensus(consensus_path, aligned_path, \n",
      "                            consensus_name, fasta_file_name, \n",
      "                            group_file_name,\n",
      "                            plurality):\n",
      "    \"\"\"\n",
      "    Run the previous functions in parallel.\n",
      "    Arguments:\n",
      "        - consensus_path: string indicating the path where the individual consensus are.\n",
      "        - aligned_path: the path were the individual aligments are.\n",
      "        - consensus_name: the name of the consensus file with all consensus sequences.\n",
      "        - fasta_file_name: the original sequences from which the consensus were made.\n",
      "        - group_file_name: the file indicating groups from cd-hit\n",
      "        - plurality: the minimal proportion of agreement that must be at a given position for\n",
      "                     the consensus to receive the most frequent base at that position.\n",
      "    \"\"\"\n",
      "    if os.path.isdir(consensus_path):\n",
      "        shutil.rmtree(consensus_path)\n",
      "    os.makedirs(consensus_path)\n",
      "    groups = make_dict_groups(group_file_name = group_file_name,\n",
      "                              fasta_file_name = fasta_file_name)\n",
      "    cmd_template = Template(\"cp $aligned_path$group_name $consensus_path$group_name\")\n",
      "    for group in groups.values():\n",
      "        if group.n_members == 1:\n",
      "            cmd = cmd_template.substitute(consensus_path = consensus_path, \n",
      "                                         aligned_path = aligned_path,\n",
      "                                         group_name = group.name)\n",
      "            os.system(cmd)\n",
      "    # Clusters are only sent to work if the number of members > 1\n",
      "    clusters = [group_name for group_name, group in groups.iteritems()\\\n",
      "                           if group.n_members > 1]\n",
      "    # Import os in all engines (nodes)\n",
      "    dview.execute(\"import os\")\n",
      "    # Send the make_consensus function to all engines\n",
      "    dview.push({\"make_consensus\":make_consensus,\n",
      "                \"read_fasta\":read_fasta})\n",
      "    # Map/Reduce\n",
      "    task = lview.map(partial(make_consensus,\n",
      "                             aligned_path = aligned_path, \n",
      "                             consensus_path = consensus_path, \n",
      "                             plurality = plurality),\n",
      "                             clusters)\n",
      "    wait_on(task)\n",
      "    if not task.successful():\n",
      "        raise Exception(\"Consensus failed!\")\n",
      "    gather_all_consensus(consensus_path = consensus_path, \n",
      "                         consensus_name = consensus_name)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 32
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Classify Sequences"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_seqs_id_from_pester():\n",
      "    \"\"\" \n",
      "    Extracts relevant information about sequences (name, full_name, acc,\n",
      "    Taxon_Level1 and Seq_Len) as given by Pester et al 2012 in the arb\n",
      "    database provided as supplementary material.\n",
      "    This function only applies for the analysis of Archaea amoA\n",
      "    \"\"\"\n",
      "    # Read data from the NDS file exported from arb\n",
      "    seq_all = pandas.read_table(\"./Pester Consensus Data/pester.nds\", \n",
      "                             header=None)\n",
      "    # Rename the columns\n",
      "    seq_all.columns = [\"name\", \"full_name\", \"taxonomy\", \"acc\", \n",
      "                       \"Taxon_Level_1\", \"Taxon_Level_2\", \"Taxon_Level_3\", \n",
      "                       \"Seq_Len\", \"Habitat\", \"unknown\"]\n",
      "    # Select rows from sequences in the tree.\n",
      "    selected_rows = seq_all.Taxon_Level_1.notnull()\n",
      "    # Select relevant columns for subsequent analysis\n",
      "    selected_columns = [\"name\", \"Taxon_Level_1\", \"Taxon_Level_2\", \"Taxon_Level_3\"]\n",
      "    seq_valid = seq_all.ix[selected_rows, selected_columns]\n",
      "    return seq_valid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Screen Oligos"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def enumerate_oligos(starts, kmer_sizes, seqs_all, look_ahead = 50):\n",
      "    \"\"\"\n",
      "    Enumerate all possible oligos (kmers) with sizes kmer_sizes from\n",
      "    the aligment.\n",
      "    \"\"\"\n",
      "    unique = {}\n",
      "    for start in starts:\n",
      "        # To account for gaps, I choose a region much bigger than kmer_size\n",
      "        seqs = [seq[start:start + look_ahead].replace(\"-\", \"\") for seq in seqs_all]\n",
      "        for kmer_size in kmer_sizes:\n",
      "            oligos = [seq[:kmer_size] for seq in seqs]\n",
      "            for count, oligo in enumerate(oligos):\n",
      "                if oligo:\n",
      "                    if not oligo in unique:\n",
      "                        unique[oligo] = [count]\n",
      "                    else:\n",
      "                        unique[oligo] += [count]\n",
      "    unique_set = {oligo:set(accs) for oligo, accs in unique.iteritems()}\n",
      "    return unique_set"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_oligos(oligos_dict, primer_conc,\n",
      "                  hairpin_tm_max, homo_tm_max,\n",
      "                  tm_max, tm_min, min_occurrence, no_3_T, \n",
      "                  no_poly_3_GC, no_poly_run, max_degen,\n",
      "                  mv_conc, dv_conc, rev):\n",
      "    \"\"\"\n",
      "    Removes oligos that don't have desirable properties.\n",
      "    \"\"\"\n",
      "    # Rescale concentration of primers\n",
      "    primer_conc_resc = primer_conc/float(max_degen)\n",
      "    # Remove oligos that occur less than min_occurrence\n",
      "    if type(oligos_dict.values()[0]) == int:\n",
      "        oligos = [oligo for oligo, value in oligos_dict.iteritems() if \\\n",
      "                                               value >= min_occurrence]\n",
      "    else:\n",
      "        oligos = [oligo for oligo, value in oligos_dict.iteritems() if \\\n",
      "                                         len(value) >= min_occurrence]\n",
      "    # It is necessary to use the reverse of the complement for the reverse primers\n",
      "    if rev:\n",
      "        oligos = [str(Seq(oligo).reverse_complement()) for oligo in oligos]\n",
      "    # Apply empirical rules\n",
      "    oligos = apply_rules(oligos, rev, no_3_T, no_poly_3_GC)\n",
      "\n",
      "    # Remove kmers with hairpin tm > threshold\n",
      "    calc_hairpin = partial(primer3.calcHairpinTm,\n",
      "                           dna_conc = primer_conc_resc,\n",
      "                           mv_conc = mv_conc, \n",
      "                           dv_conc = dv_conc)\n",
      "    hairpin = map(calc_hairpin, oligos)\n",
      "    oligos = [oligo for oligo, hp_tm in zip(oligos, hairpin) if hp_tm <= hairpin_tm_max]\n",
      "\n",
      "    # Remove kmers with homodimers tm > threshold\n",
      "    calc_homo_tm = partial(primer3.calcHomodimerTm,    \n",
      "                           dna_conc = primer_conc_resc,\n",
      "                           mv_conc = mv_conc, \n",
      "                           dv_conc = dv_conc)\n",
      "    homo_tm = map(calc_homo_tm, oligos)\n",
      "    oligos = [oligo for oligo, homo_tm in zip(oligos, homo_tm) if homo_tm <= homo_tm_max]\n",
      "\n",
      "    # Remove kmers with poly runs\n",
      "    if no_poly_run:\n",
      "        polys = [\"AAAA\", \"TTTT\", \"GGGG\", \"CCCC\"]\n",
      "        find_poly = lambda x: not any([True for poly in polys if poly in x])\n",
      "        oligos = filter(find_poly, oligos)\n",
      "    \n",
      "    # Remove primers with tm above threshold\n",
      "    calc_tm = partial(primer3.calcTm,                           \n",
      "                      dna_conc = primer_conc_resc,\n",
      "                      mv_conc = mv_conc, \n",
      "                      dv_conc = dv_conc)\n",
      "    tms = map(calc_tm, oligos)\n",
      "    for oligo, tm in zip(oligos, tms):\n",
      "        if tm < tm_min or tm > tm_max:\n",
      "            oligos.remove(oligo)\n",
      "        else:\n",
      "            pass\n",
      "    # For compatibility with the dictionaries, I will return the rev primers for their original \n",
      "    if rev:\n",
      "        oligos = [str(Seq(oligo).reverse_complement()) for oligo in oligos]\n",
      "    return oligos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def apply_rules(oligos, rev, no_3_T, no_poly_3_GC):\n",
      "    \"\"\"\n",
      "    Apply some empirical rules for primer design.\n",
      "    \"\"\"\n",
      "    # I invert the oligo when it is the reverse so that I can treat the 3 terminal equally\n",
      "    if no_poly_3_GC:\n",
      "        find_poly_GC = lambda x: not any([True for poly in [\"GGG\", \"CCC\"] if poly in x[-3:]])\n",
      "        oligos = filter(find_poly_GC, oligos)\n",
      "    if no_3_T:\n",
      "        oligos = filter(lambda x: not x[-1] == \"T\", oligos)\n",
      "    return oligos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def discard_redundant(oligo_series, max_degen, rev, \n",
      "                      primer_conc, verbose,  min_diff,\n",
      "                      mv_conc, dv_conc):\n",
      "    \"\"\"\n",
      "    From reduntant oligos (oligos that detects exactly the same sequences) select those who have\n",
      "    highest melting temperature and keep the remaining in a dictionary for further reuse if the \n",
      "    representant is discarded for any reason.\n",
      "    \"\"\"\n",
      "    # Rescale primer concentration\n",
      "    primer_conc_resc = primer_conc/float(max_degen)\n",
      "    \n",
      "    to_remove = []\n",
      "    if rev:\n",
      "        oligos = [str(Seq(oligo).reverse_complement()) for oligo in oligo_series.index]\n",
      "    else:\n",
      "        oligos = [oligo for oligo in oligo_series.index]\n",
      "    redundant = {}\n",
      "    calc_tm = partial(primer3.calcTm,                           \n",
      "                  dna_conc = primer_conc_resc,\n",
      "                  mv_conc = mv_conc, \n",
      "                  dv_conc = dv_conc)\n",
      "    tms = map(calc_tm, oligos)\n",
      "    tms = {oligo:tm for tm, oligo in zip(tms, oligo_series.index)}\n",
      "    for count, base_primer in enumerate(oligo_series.index):\n",
      "        if verbose:\n",
      "            clear_output()\n",
      "            print \"Iteration %d of %d\" % (count, len(oligo_series))\n",
      "            sys.stdout.flush()\n",
      "        if base_primer in to_remove:\n",
      "            continue\n",
      "        for primer in oligo_series.index:\n",
      "            if primer == base_primer:\n",
      "                continue\n",
      "            if primer in to_remove:\n",
      "                continue\n",
      "            union =  oligo_series[base_primer].union(oligo_series[primer])\n",
      "            diff = len(union) - len(oligo_series[base_primer])\n",
      "            size_diff = len(oligo_series[base_primer]) - len(oligo_series[primer])\n",
      "            if numpy.abs(diff) <= min_diff and size_diff <= min_diff:\n",
      "                # If two oligos match the same sequences, keep the one with highest Tm\n",
      "                if tms[primer] < tms[base_primer]:\n",
      "                    to_remove += [primer]\n",
      "                    # Keep a record of the redundant oligos because they may be reused if the\n",
      "                    # one chose here is discarded later\n",
      "                    if not base_primer in redundant:\n",
      "                        redundant[base_primer] = [primer]\n",
      "                    else:\n",
      "                        redundant[base_primer] += [primer]\n",
      "                    # If the primer that was removed contains other redundant primers\n",
      "                    # add them to the dict as well in the key corresponding to the selected primer\n",
      "                    if primer in redundant:\n",
      "                        redundant[base_primer] += redundant[primer]\n",
      "                else:\n",
      "                    to_remove += [base_primer]\n",
      "                    if not primer in redundant:\n",
      "                        redundant[primer] = [base_primer]\n",
      "                    else:\n",
      "                        redundant[primer] += [base_primer]\n",
      "                    if base_primer in redundant:\n",
      "                        redundant[primer] += redundant[base_primer]\n",
      "                    break\n",
      "    to_keep = [index for index in oligo_series.index if not index in to_remove]\n",
      "    return {\"oligo_series\":oligo_series[to_keep], \"redundant\":redundant}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 40
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_valid_positions(seqs_all, max_gap_prop):\n",
      "    \"\"\"\n",
      "    Find positions in the aligment that are not mostly gaps.\n",
      "    \"\"\"\n",
      "    positions = zip(*seqs_all)\n",
      "    count_gaps = lambda x: len([base for base in x if base == \"-\"])\n",
      "    gaps = map(count_gaps, positions)\n",
      "    max_gaps = len(seqs_all) * max_gap_prop\n",
      "    pos = range(len(positions))\n",
      "    valid_pos = filter(lambda x: x[1] <= max_gaps, zip(pos, gaps))\n",
      "    valid_pos = [p[0] for p in valid_pos]\n",
      "    return valid_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def enumerate_positions_for_screen(fasta_file, kmer_sizes, step, \n",
      "                                   max_gap_prop):\n",
      "    \"\"\"\n",
      "    Calculates the positions where oligos should be enumerated based on the\n",
      "    step parameter.\n",
      "    \"\"\"\n",
      "    # Select valid columns\n",
      "    records = read_fasta(fasta_file)\n",
      "    n_seq = float(len(records))\n",
      "    seqs_all = [str(record.seq) for record in records]\n",
      "    valid_pos = find_valid_positions(seqs_all, max_gap_prop)\n",
      "    \n",
      "    # Define positions were oligos will be enumerated\n",
      "    last_pos = valid_pos[ :-min(kmer_sizes)]\n",
      "    \n",
      "    pos = [p for p in valid_pos if p < last_pos]\n",
      "    starts = [pos[i] for i in range(0, len(pos), step)]\n",
      "    return (starts, seqs_all, n_seq)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 41
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def enumerate_kmers_screen(start, starts, seqs_all, n_seq, kmer_sizes, \n",
      "                           hairpin_tm_max, homo_tm_max,\n",
      "                           tm_max, tm_min, min_occurrence, no_3_T, \n",
      "                           no_poly_3_GC, max_degen, no_poly_run,\n",
      "                           primer_conc, min_diff, mv_conc, dv_conc, look_ahead):\n",
      "    \"\"\"\n",
      "    Apply the functions above to enumerate all possible oligos that match some criteria\n",
      "    specified in its arguments along a given alignemnt. This is used for screening positions\n",
      "    in the alignment that might be useful for designing primers.\n",
      "    \"\"\"\n",
      "    # Rescale primer concentration\n",
      "    primer_conc_resc = primer_conc/float(max_degen)\n",
      "    # After the middle of the alignment, primers will be tested as reverse\n",
      "    rev = False\n",
      "    if start > starts[len(starts) / 2]:\n",
      "        rev = True\n",
      "    unique_kmers = enumerate_oligos(starts = [start], \n",
      "                                    kmer_sizes = kmer_sizes, \n",
      "                                    seqs_all = seqs_all,\n",
      "                                    look_ahead = look_ahead)\n",
      "    \n",
      "    kmers = filter_oligos(oligos_dict = unique_kmers, \n",
      "                          rev = rev, \n",
      "                          primer_conc = primer_conc,\n",
      "                          hairpin_tm_max = hairpin_tm_max, \n",
      "                          homo_tm_max = homo_tm_max, \n",
      "                          tm_max = tm_max, \n",
      "                          tm_min = tm_min, \n",
      "                          min_occurrence = min_occurrence, \n",
      "                          no_3_T = no_3_T, \n",
      "                          no_poly_3_GC = no_poly_3_GC, \n",
      "                          max_degen = max_degen, \n",
      "                          no_poly_run = no_poly_run,\n",
      "                          mv_conc = mv_conc, \n",
      "                          dv_conc = dv_conc)\n",
      "\n",
      "    unique_kmers = {i:unique_kmers[i] for i in kmers}\n",
      "    kmers_series = pandas.Series(unique_kmers)\n",
      "    cover = kmers_series.map(len).sort(inplace = False, ascending = False)\n",
      "    kmers_series = kmers_series[cover.index]\n",
      "    not_redundant = discard_redundant(kmers_series, \n",
      "                                      max_degen = max_degen, \n",
      "                                      rev = rev, \n",
      "                                      primer_conc = primer_conc, \n",
      "                                      verbose = False,\n",
      "                                      min_diff = min_diff,\n",
      "                                      mv_conc = mv_conc,\n",
      "                                      dv_conc = dv_conc)\n",
      "    \n",
      "    kmers_series = not_redundant[\"oligo_series\"]\n",
      "    unique_kmers = {kmer:unique_kmers[kmer] for kmer in \\\n",
      "                                            kmers_series.index}\n",
      "    # Selected the n (max_degen) best oligos\n",
      "    sequences_detected = kmers_series.map(len)\n",
      "    sequences_detected.sort(ascending = False)\n",
      "    best_primers = sequences_detected.index[:max_degen]\n",
      "    detected = set()\n",
      "    for oligo in best_primers:\n",
      "        detected = detected.union(kmers_series[oligo])\n",
      "    coverage = len(detected) / n_seq\n",
      "    unique_kmers = {key:unique_kmers[key] for key in best_primers}\n",
      "    \n",
      "    calc_tm = partial(primer3.calcTm,                           \n",
      "                      dna_conc = primer_conc_resc,\n",
      "                      mv_conc = mv_conc, \n",
      "                      dv_conc = dv_conc)\n",
      "    tm = map(calc_tm, unique_kmers.keys())\n",
      "    if len(tm):\n",
      "        tms_median = numpy.median(tm)\n",
      "        tms_10 = numpy.percentile(tm, 10)\n",
      "        tms_90 = numpy.percentile(tm, 90)\n",
      "    else:\n",
      "        tms_median, tms_10, tms_90 = (0, 0, 0)\n",
      "    # After these filtering, calculate coverage and richness of the most\n",
      "    # abundant oligos\n",
      "    richness = len(unique_kmers)\n",
      "    return ((start, {\"Richness\":richness, \n",
      "                    \"Coverage\":coverage, \n",
      "                    \"Median_Tm\":tms_median,\n",
      "                    \"Tm_10\":tms_10,\n",
      "                    \"Tm_90\":tms_90}), \n",
      "             unique_kmers, tm)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def screen_oligos(fasta_file, kmer_sizes, hairpin_tm_max = 35, homo_tm_max = 35,\n",
      "                  tm_max = 65, tm_min = 50, min_occurrence = 5, no_3_T = True, \n",
      "                  no_poly_3_GC = True, max_degen = 60, no_poly_run = True,\n",
      "                  step = 3, primer_conc = 200, max_gap_prop = .1, \n",
      "                  min_diff = 0, mv_conc = 50, dv_conc = 1.5, look_ahead = 50):\n",
      "    \"\"\"\n",
      "    Apply the function enumerate_kmers_screen in parallel for enumerating all possible oligos that match some criteria\n",
      "    specified in its arguments along a given alignment. This is used for screening positions\n",
      "    in the alignment that might be useful for designing primers.\n",
      "    Arguments:\n",
      "        -fasta_file: a string given the name of the fasta file with aligned sequences to be used for enumeration of oligos;\n",
      "        -kmer_sizes: a list of integers with the desired size of oligos;\n",
      "        -hairpin_tm_max: maximal hairpin melting temperature allowed (in Celsius degrees);\n",
      "        -homo_tm_max: maximal homodimer melting temperature allowed (in Celsius degrees);\n",
      "        -tm_max: maximal melting temperature allowed for a oligo (in Celsius degrees);\n",
      "        -tm_min: minimal melting temperature allowed for a oligo (in Celsius degrees);\n",
      "        -min_occurrence: integer. Minimal allowed occurrence of a oligo along all sequences;\n",
      "        -no_3_T: boolean. Should oligos with a T in the 3' end be discarded?\n",
      "        -no_poly_3_GC: boolean. Should oligos with three G's of C's in the 3' end be discarded?\n",
      "        -max_degen: the maximal number of subprimers desired. This will also be used to rescale the oligo concentration.\n",
      "        -no_poly_run: boolean. Should oligos with four or more runs of the same bases be discarded?\n",
      "        -step: distance between positions in the aligment from which primers should be enumerated. A step of 1 implies that\n",
      "               all positions will be used.\n",
      "        -primer_conc: total primer concentration in nM. This concentration will be rescaled automatically by the max_degen.\n",
      "        -max_gap_prop: float. Maximal proportion of gaps allowed for any position where oligos will be enumerated.\n",
      "        -min_diff: minimal difference of sequences detected for a oligo to be considered redundant. This parameter should be\n",
      "                   kept at its default value unless you have strong reasons to change it.\n",
      "        -mv_conc: monovalent ions concentration in mM.\n",
      "        -dv_conc: divalent ions conentration in mM.\n",
      "    \"\"\"\n",
      "    starts, seqs_all, n_seq = enumerate_positions_for_screen(\\\n",
      "                                    fasta_file,\n",
      "                                    kmer_sizes = kmer_sizes, \n",
      "                                    max_gap_prop = max_gap_prop,\n",
      "                                    step = step)\n",
      "    kwargs = {\"starts\":starts, \n",
      "            \"seqs_all\":seqs_all, \n",
      "            \"n_seq\":n_seq, \n",
      "            \"kmer_sizes\":kmer_sizes, \n",
      "            \"hairpin_tm_max\":hairpin_tm_max, \n",
      "            \"homo_tm_max\":homo_tm_max,\n",
      "            \"tm_max\":tm_max, \n",
      "            \"tm_min\":tm_min, \n",
      "            \"min_occurrence\":min_occurrence, \n",
      "            \"no_3_T\":no_3_T, \n",
      "            \"no_poly_3_GC\":no_poly_3_GC, \n",
      "            \"max_degen\":max_degen, \n",
      "            \"no_poly_run\":no_poly_run,\n",
      "            \"primer_conc\":primer_conc,\n",
      "            \"min_diff\":min_diff,\n",
      "            \"mv_conc\":mv_conc,\n",
      "            \"dv_conc\":dv_conc,\n",
      "            \"look_ahead\":look_ahead}\n",
      "    p = partial(enumerate_kmers_screen, **kwargs)\n",
      "    dview.push({\"enumerate_kmers_screen\":enumerate_kmers_screen,\n",
      "               \"discard_redundant\":discard_redundant,\n",
      "               \"filter_oligos\":filter_oligos,\n",
      "               \"enumerate_oligos\":enumerate_oligos,\n",
      "               \"apply_rules\":apply_rules})\n",
      "    task = lview.map(p, starts, chunksize = 20)\n",
      "    wait_on(task)\n",
      "    pos = {res[0][0]:res[0][1] for res in task.result}\n",
      "    unique_kmers = [(res[0][0], res[1]) for res in task.result]\n",
      "    data = pandas.DataFrame(pos).T\n",
      "    data[\"Pos\"] = data.index\n",
      "    return {\"data\":data, \"unique_kmers\":unique_kmers}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 43
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def combine_positions(positions):\n",
      "    all = []\n",
      "    for pos in positions:\n",
      "        all += unique_sets[pos]\n",
      "    return set(all)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Enumerate oligos using specified positions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Some of the functions used in this section were defined in the previous section"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def unite_pairs(pair):\n",
      "    set_pair = fwd_unique[pair[0]].intersection(rev_unique[pair[1]])\n",
      "    return (pair, set_pair)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def enumerate_pairs(fwd_unique, rev_unique):\n",
      "    \"\"\"\n",
      "    Combine fwd and rev primers as pairs and return it as a dict where the key is the pair itself\n",
      "    and the values are the set of sequences detected by the pair.\n",
      "    \"\"\"\n",
      "    pairs_oligos = list(product(fwd_unique.index, rev_unique.index))\n",
      "    dview.push({\"fwd_unique\":fwd_unique,\n",
      "                \"rev_unique\":rev_unique,\n",
      "                \"unite_pairs\":unite_pairs})\n",
      "    if not len(pairs_oligos):\n",
      "        print  \"No primers were found!\"\n",
      "        return\n",
      "    task = lview.map(unite_pairs, pairs_oligos, chunksize = 1000)\n",
      "    wait_on(task)\n",
      "    pairs = {pair:set for pair, set in task.result}\n",
      "    return pairs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_pair(fwd, rev, max_delta, max_tm_ht, dv_conc, mv_conc, primer_conc):\n",
      "    \"\"\"\n",
      "    Tests whether or not a pair o primer is compatible\n",
      "    \"\"\"\n",
      "    rev_comp = str(Seq(rev).reverse_complement())\n",
      "    fwd_tm = primer3.calcTm(fwd, mv_conc = mv_conc,\n",
      "                            dv_conc = dv_conc,\n",
      "                            dna_conc = primer_conc)\n",
      "    rev_tm = primer3.calcTm(rev_comp, mv_conc = mv_conc,\n",
      "                            dv_conc = dv_conc,\n",
      "                            dna_conc = primer_conc)\n",
      "    is_delta_high = np.abs(fwd_tm - rev_tm) > max_delta\n",
      "    ht_tm = primer3.calcHeterodimerTm(fwd, rev_comp, \n",
      "                                      mv_conc = mv_conc,\n",
      "                                      dv_conc = dv_conc,\n",
      "                                      dna_conc = primer_conc)\n",
      "    is_heterodimer = ht_tm > max_tm_ht\n",
      "    return is_delta_high or is_heterodimer"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def update_redundancy(redundancy, new_key, rev = False):\n",
      "    oligo_key = \"rev\" if rev else \"fwd\"\n",
      "    for key, value in redundancy[oligo_key].iteritems():\n",
      "        if new_key in value:\n",
      "            redundancy[oligo_key][new_key] = redundancy[oligo_key][key]\n",
      "    return redundancy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_heterodimer(fwd, rev, included_fwd, included_rev_comp, max_tm_ht, mv_conc, dv_conc, primer_conc):\n",
      "    if not len(included_rev_comp) and not len(included_fwd):\n",
      "        return False\n",
      "    rev_comp = str(Seq(rev).reverse_complement())\n",
      "    for oligo in included_rev_comp + included_fwd:\n",
      "        for candidate in [rev_comp, fwd]:\n",
      "            ht_tm = primer3.calcHeterodimerTm(oligo, \n",
      "                                              candidate, \n",
      "                                              mv_conc = mv_conc,\n",
      "                                              dv_conc = dv_conc,\n",
      "                                              dna_conc = primer_conc)\n",
      "            if ht_tm >= max_tm_ht:\n",
      "                return True"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def select_pairs(fwd_dict, rev_dict, redundancy_dict, pairs, max_degen = 100, max_delta = 5, \n",
      "                 max_tm_ht = 35, min_increase = 5, primer_conc=200, mv_conc = 50, dv_conc = 1.5):\n",
      "    \"\"\"\n",
      "    Select primers as pairs based on their coverage and compatibility.\n",
      "    Arguments:\n",
      "        -fwd_dict: a dict of forward primers as returned by the function enumerate_primers;\n",
      "        -rev_dict: a dict of reverse primers as returned by the function enumerate_primers;\n",
      "        -redundancy_dict: a dict of primers grouped by their redundancy as returned by enumerate_primers;\n",
      "        -pairs: a dict with pairs of primers as keys and the sequences detected by the pair as values. This\n",
      "                dict is created by the function enumerate_primers;\n",
      "        -max_degen: maximal number of subprimers to be kept;\n",
      "        -max_delta: maximal absolute difference in melting temperature between primers in a pair. \n",
      "        -max_tm_ht: maximal heterodimer melting temperature;\n",
      "        -min_increase: minimal number of new sequences detected for a candidate pair to be selected;\n",
      "    \"\"\"\n",
      "    best = set()\n",
      "    included_fwd = []\n",
      "    included_rev = []\n",
      "    included_rev_comp = []\n",
      "    pairs_series = pandas.Series(pairs)\n",
      "    cover = pairs_series.map(len).sort(inplace = False, ascending = False)\n",
      "    pairs_series = pairs_series[cover.index]\n",
      "    # Rescale concentration of primers\n",
      "    primer_conc_resc = primer_conc/float(max_degen)\n",
      "    # Increase the min_increase for the first iterations\n",
      "    current_min_increase = min_increase + 100\n",
      "    while True:\n",
      "        sys.stdin.readline() # Just to display the results in real time.\n",
      "        increase = 0\n",
      "        to_del = []\n",
      "        reject_pair = False\n",
      "        fwd_degen_reached = len(included_fwd) > max_degen\n",
      "        rev_degen_reached = len(included_rev) > max_degen\n",
      "        for fwd, rev in pairs_series.index:\n",
      "            # If the number of sequences detected by the pair is smaller than min_increase, \n",
      "            # delete it and go to next iteration.\n",
      "            if len(pairs[(fwd, rev)]) < min_increase:\n",
      "                to_del += [(fwd, rev)]\n",
      "                continue\n",
      "            will_degen_exceed_fwd =  (fwd not in included_fwd) and fwd_degen_reached \n",
      "            will_degen_exceed_rev =  (rev not in included_rev) and rev_degen_reached\n",
      "            is_not_compatible = filter_pair(fwd = fwd, rev = rev, max_delta = max_delta, \n",
      "                                            max_tm_ht = max_tm_ht, dv_conc = dv_conc, \n",
      "                                            mv_conc = mv_conc, primer_conc = primer_conc_resc)\n",
      "            if is_not_compatible:\n",
      "                # When a pair is not compatible, try to look for redundant oligos that are compatible\n",
      "                new_pair = reuse_redundant(fwd = fwd, rev = rev, redundancy_dict = redundancy_dict, \n",
      "                                           max_delta = max_delta, max_tm_ht = max_tm_ht,\n",
      "                                           dv_conc = dv_conc, mv_conc = mv_conc, \n",
      "                                           primer_conc = primer_conc_resc)\n",
      "                if new_pair:\n",
      "                    new_fwd, new_rev = new_pair\n",
      "                    pairs[(new_fwd, new_rev)] = pairs[(fwd, rev)]\n",
      "                    fwd, rev = new_fwd, new_rev\n",
      "                    is_not_compatible = False\n",
      "            # If a pair is incompatible and no substitute could be found or \n",
      "            # if the degeration was reached, delete the pair and go to next iteration.\n",
      "            if will_degen_exceed_fwd or will_degen_exceed_rev or is_not_compatible:\n",
      "                to_del += [(fwd, rev)]\n",
      "                continue\n",
      "            # If both primers were already included in previous pairs, add the detected sequences \n",
      "            # to the set of detected sequences, delete the pair, and go to next iteration.\n",
      "            if fwd in included_fwd and rev in included_rev:\n",
      "                best = best.union(pairs[(fwd, rev)])\n",
      "                to_del += [(fwd, rev)]\n",
      "                continue\n",
      "            is_heterodimer = test_heterodimer(fwd = fwd, rev = rev, \n",
      "                                              included_fwd = included_fwd, \n",
      "                                              included_rev_comp = included_rev_comp, \n",
      "                                              max_tm_ht = max_tm_ht, \n",
      "                                              mv_conc = mv_conc, dv_conc = dv_conc, \n",
      "                                              primer_conc = primer_conc_resc)\n",
      "            if is_heterodimer:\n",
      "                to_del += [(fwd, rev)]\n",
      "                continue\n",
      "            # If a pair survived the previous conditions, test it.\n",
      "            union = pairs[fwd, rev].union(best)\n",
      "            increase = len(union) - len(best)\n",
      "            # If the pair increases the coverage, add it to the included oligos list\n",
      "            # delete the pair and stop this internal loop\n",
      "            if increase >= current_min_increase:\n",
      "                best = best.union(pairs[(fwd, rev)])\n",
      "                if not fwd in included_fwd:\n",
      "                    included_fwd += [fwd]\n",
      "                if not rev in included_rev:\n",
      "                    included_rev += [rev]\n",
      "                    included_rev_comp = [str(Seq(rev_i).reverse_complement()) for rev_i in included_rev_comp]\n",
      "                to_del += [(fwd, rev)]\n",
      "                break\n",
      "            elif increase < min_increase:\n",
      "                to_del += [(fwd, rev)]\n",
      "        # If no pair gave a high enough increase, reduce the current_min_increase by 20\n",
      "        # until reach the min_increase specified by the user\n",
      "        if increase < current_min_increase:\n",
      "            current_min_increase -= 20\n",
      "            if current_min_increase < min_increase:\n",
      "                break\n",
      "        if fwd_degen_reached and rev_degen_reached:\n",
      "            break\n",
      "        if to_del:\n",
      "            to_keep = [idx for idx in pairs_series.index if not idx in to_del]\n",
      "            pairs = {key:pairs[key] for key in to_keep}\n",
      "            pairs_series = pairs_series[to_keep]\n",
      "        clear_output()\n",
      "        print \"Total Coverage: %d\" % (len(best))\n",
      "        print \"Forward Degeneration: %d\" % (len(included_fwd))\n",
      "        print \"Reverse Degeneration: %d\" % (len(included_rev))\n",
      "        print \"Remaing pairs: %d\" % (len(pairs))\n",
      "        sys.stdout.flush()\n",
      "    return {\"fwd\":included_fwd,\n",
      "        \"rev\":included_rev,\n",
      "        \"covered\":best}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def enumerate_primers(target_file_name,\n",
      "                      fwd_starts,\n",
      "                      rev_starts,\n",
      "                      hairpin_tm_max = 30, \n",
      "                      primer_conc = 200, \n",
      "                      homo_tm_max = 30,\n",
      "                      kmer_sizes = [18, 19, 20, 21, 23, 24, 25, 26, 27, 28],\n",
      "                      tm_min = 55,\n",
      "                      tm_max = 60,\n",
      "                      min_occurrence = 10,\n",
      "                      no_3_T = True,\n",
      "                      no_poly_3_GC = True,\n",
      "                      no_poly_run = True,\n",
      "                      max_degen = 60,\n",
      "                      mv_conc=50, \n",
      "                      dv_conc=1.5,\n",
      "                      look_ahead = 50):\n",
      "    \"\"\"\n",
      "    Enumerates forward and reverse primers from two regions of an alignment and filters them according to user-defined criteria.\n",
      "    Arguments:\n",
      "        -target_file_name: a string given the name of the fasta file with aligned sequences to be used for enumeration of oligos;\n",
      "        -fwd_starts: a list of integers giving the starting positions for enumerating forward primers;\n",
      "        -rev_starts: a list of integers giving the starting positions for enumerating reverse primers;\n",
      "        -hairpin_max_tm: maximal hairpin melting temperature allowed (in Celsius degrees);\n",
      "        -primer_conc: total primer concentration in nM. This concentration will be rescaled automatically by the max_degen.\n",
      "        -homo_tm_max: maximal homodimer melting temperature allowed (in Celsius degrees);\n",
      "        -kmer_sizes: a list of integers with the desired size of oligos;\n",
      "        -tm_min: minimal melting temperature allowed for a oligo (in Celsius degrees);\n",
      "        -tm_max: maximal melting temperature allowed for a oligo (in Celsius degrees);\n",
      "        -min_occurrence: integer. Minimal allowed occurrence of a oligo along all sequences;\n",
      "        -no_3_T: boolean. Should oligos with a T in the 3' end be discarded?\n",
      "        -no_poly_3_GC: boolean. Should oligos with three G's of C's in the 3' end be discarded?\n",
      "        -max_degen: the maximal number of subprimers desired. This will also be used to rescale the oligo concentration.\n",
      "        -no_poly_run: boolean. Should oligos with four or more runs of the same bases be discarded?\n",
      "        -step: distance between positions in the aligment from which primers should be enumerated. A step of 1 implies that\n",
      "               all positions will be used.\n",
      "        -mv_conc: monovalent ions concentration in mM.\n",
      "        -dv_conc: divalent ions concentration in mM.\n",
      "    \"\"\"\n",
      "    records = read_fasta(target_file_name)\n",
      "    seqs_all = [str(record.seq) for record in records]\n",
      "    # Enumerate oligos\n",
      "    fwd_unique = enumerate_oligos(starts = fwd_starts,\n",
      "                                  kmer_sizes = kmer_sizes, \n",
      "                                  seqs_all = seqs_all,\n",
      "                                  look_ahead = look_ahead)\n",
      "    rev_unique = enumerate_oligos(starts = rev_starts,\n",
      "                                  kmer_sizes = kmer_sizes, \n",
      "                                  seqs_all = seqs_all,\n",
      "                                  look_ahead = look_ahead)\n",
      "    # Filter oligos\n",
      "    fwd_oligos = filter_oligos(fwd_unique, \n",
      "                               rev = False, \n",
      "                               hairpin_tm_max = hairpin_tm_max, \n",
      "                               homo_tm_max = homo_tm_max, \n",
      "                               tm_max = tm_max, \n",
      "                               tm_min = tm_min,\n",
      "                               min_occurrence = min_occurrence, \n",
      "                               primer_conc = primer_conc,\n",
      "                               no_3_T = no_3_T, \n",
      "                               no_poly_3_GC = no_poly_3_GC,\n",
      "                               no_poly_run = no_poly_run,\n",
      "                               max_degen = max_degen,\n",
      "                               mv_conc = mv_conc, \n",
      "                               dv_conc = dv_conc)\n",
      "    rev_oligos = filter_oligos(rev_unique, \n",
      "                               rev = True, \n",
      "                               hairpin_tm_max = hairpin_tm_max, \n",
      "                               homo_tm_max = homo_tm_max, \n",
      "                               tm_max = tm_max, \n",
      "                               tm_min = tm_min,\n",
      "                               min_occurrence = min_occurrence, \n",
      "                               primer_conc = primer_conc,\n",
      "                               no_3_T = no_3_T, \n",
      "                               no_poly_3_GC = no_poly_3_GC, \n",
      "                               no_poly_run = no_poly_run,\n",
      "                               max_degen = max_degen,\n",
      "                               mv_conc = mv_conc, \n",
      "                               dv_conc = dv_conc)\n",
      "    # Remove redundancy\n",
      "    fwd_unique = pandas.Series({oligo:fwd_unique[oligo] for oligo in fwd_oligos})\n",
      "    fwd_reduced = discard_redundant(fwd_unique,\n",
      "                                    max_degen = max_degen, \n",
      "                                    rev = False, \n",
      "                                    primer_conc = primer_conc, \n",
      "                                    verbose = False,\n",
      "                                    min_diff = 0,\n",
      "                                    mv_conc = mv_conc,\n",
      "                                    dv_conc = dv_conc)\n",
      "    fwd_unique = fwd_reduced[\"oligo_series\"]\n",
      "    fwd_cover = fwd_unique.map(len).sort(inplace = False, ascending = False)\n",
      "    rev_unique = pandas.Series({oligo:rev_unique[oligo] for oligo in rev_oligos})\n",
      "    rev_reduced = discard_redundant(rev_unique,\n",
      "                                    max_degen = max_degen, \n",
      "                                    rev = True, \n",
      "                                    primer_conc = primer_conc, \n",
      "                                    verbose = False,\n",
      "                                    min_diff = 0,\n",
      "                                    mv_conc = mv_conc,\n",
      "                                    dv_conc = dv_conc)\n",
      "    redundancy = {\"rev\":rev_reduced[\"redundant\"], \"fwd\":fwd_reduced[\"redundant\"]}\n",
      "    # Calculate coverage\n",
      "    rev_unique = rev_reduced[\"oligo_series\"]\n",
      "    rev_cover = rev_unique.map(len).sort(inplace = False, ascending = False)\n",
      "    # Make all possible combinations of primers as pairs\n",
      "    pairs = enumerate_pairs(fwd_unique, rev_unique)\n",
      "    if not pairs:\n",
      "        return\n",
      "    all_pairs = set()\n",
      "    for pair in pairs.values():\n",
      "        all_pairs = all_pairs.union(pair)\n",
      "    all_fwd = set()\n",
      "    for fwd in fwd_unique.values:\n",
      "        all_fwd = all_fwd.union(fwd)\n",
      "    all_rev = set()\n",
      "    for rev in rev_unique.values:\n",
      "        all_rev = all_rev.union(rev)\n",
      "    seqs_detected = all_fwd.intersection(all_rev)\n",
      "    # Report results\n",
      "    print \"Coverage of all fwd: %d \" % len(all_fwd)\n",
      "    print \"Coverage of all rev: %d \" % len(all_rev)\n",
      "    print \"Joint coverage of fwd and rev: %d\" % len(all_fwd.intersection(all_rev))\n",
      "    print \"Max possible coverage: %d\" % len(all_pairs)\n",
      "    print \"Number of Foward Oligos: %d\" % len(fwd_unique)\n",
      "    print \"Number of Reverse Oligos: %d\" % len(rev_unique)\n",
      "    return (fwd_unique, rev_unique, redundancy, pairs, seqs_detected)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def increase_degeneracy(included_oligos, redundancy, oligo_series, min_increase):\n",
      "    \"\"\"\n",
      "    After all pairs of primers have been exhausted, this function tries to add new primers independently\n",
      "    if the degeneracy is below the maximum specified by the user. This is done in the hope that these primers\n",
      "    will pair with other primers when they have one or more mismatch with the template sequence.\n",
      "    Arguments:\n",
      "        -included_oligos: oligos already included as returned by the function select pairs;\n",
      "        -redundancy: the dict of oligo groups as returned by enumerate_primers;\n",
      "        -oligo_series: a pandas series of oligos detected by each primer as returned by enumerate_primers;\n",
      "        -min_increase: minimal number of new sequences detected for a new primer to be detected.\n",
      "    \"\"\"\n",
      "    # Some oligos in the included oligos are not in the key of the dict redundancy. When that is the case\n",
      "    # I have to look for it in the values of this dict. The purpose is to have a set of all detected sequences\n",
      "    found = []\n",
      "    for oligo in included_oligos:\n",
      "        if oligo in oligo_series.index:\n",
      "            accs = oligo_series[oligo]\n",
      "        else:\n",
      "            for value in redundancy.values():\n",
      "                if oligo in value:\n",
      "                    for oligo in value:\n",
      "                        if oligo in oligo_series.index:\n",
      "                            accs = oligo_series[key]\n",
      "                            break\n",
      "                        else:\n",
      "                            pass\n",
      "                    break\n",
      "        for acc in accs:\n",
      "            found += [acc]\n",
      "    found = set(found)\n",
      "    not_included = list(set(oligo_series.index) - set(included_oligos))\n",
      "    while not_included and \\\n",
      "          len(included_oligos) < max_degen:\n",
      "        unions = pandas.Series()\n",
      "        for oligo in not_included:\n",
      "            unions[oligo] = len(found.union(oligo_series[oligo]))\n",
      "        unions = unions - len(found)\n",
      "        increase = unions.max()\n",
      "        best_oligo = unions.idxmax()\n",
      "        if increase > min_increase:\n",
      "            found = found.union(oligo_series[best_oligo])\n",
      "            included_oligos += [best_oligo]\n",
      "            not_included.remove(best_oligo)\n",
      "        else:\n",
      "            break\n",
      "        \"Perfect matches are %d.\" % len(found)\n",
      "    return included_oligos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def reuse_redundant(fwd, rev, redundancy_dict, max_delta, max_tm_ht,\n",
      "                                           dv_conc, mv_conc, primer_conc):\n",
      "    if fwd in redundancy_dict[\"fwd\"]:\n",
      "        fwd_list = redundancy_dict[\"fwd\"][fwd]\n",
      "    else:\n",
      "        fwd_list = [fwd]\n",
      "    if rev in redundancy_dict[\"rev\"]:\n",
      "        rev_list = redundancy_dict[\"rev\"][rev]\n",
      "    else:\n",
      "        rev_list = [rev]\n",
      "    for fwd in fwd_list:\n",
      "        for rev in rev_list:\n",
      "            is_compatible = not filter_pair(fwd = fwd, rev = rev, max_delta = max_delta, \n",
      "                                            max_tm_ht = max_tm_ht, dv_conc = dv_conc, \n",
      "                                            mv_conc = mv_conc, primer_conc = primer_conc)\n",
      "            if is_compatible:\n",
      "                return (fwd, rev)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 49
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def test_heterodimers_post(fwds, revs_comp, primer_conc, mv_conc, dv_conc, max_ht_tm):\n",
      "    \"\"\"\n",
      "    Tests if there are hereterodimers among the primers returned by select_pairs.\n",
      "    Arguments:\n",
      "        -best: the object returned by select_pairs;\n",
      "        -max_degen: maximal degeneration allowed. Used to rescaled primer concentration.\n",
      "        -mv_conc: monovalent ions concentration in mM;\n",
      "        -dv_conc: divalent ions concentration in mM.\n",
      "    \"\"\"\n",
      "    # Rescale primer concentration\n",
      "    degeneracy = max(len(fwds), len(revs_comp))\n",
      "    primer_conc_resc = primer_conc/float(degeneracy)\n",
      "    pairs = list(product(fwds, revs_comp))\n",
      "    calc_ht_tm = partial(primer3.calcHeterodimerTm, \n",
      "                         dna_conc = primer_conc_resc,\n",
      "                         mv_conc = mv_conc, \n",
      "                         dv_conc = dv_conc)\n",
      "    hetero_tms = map(lambda x: calc_ht_tm(x[0], x[1]) > max_ht_tm, pairs)\n",
      "    if any(hetero_tms):\n",
      "        print \"Heterodimers found!\" # I will improve this later\n",
      "        print [pair for count, pair in enumerate(pairs) if hetero_tms[count]]\n",
      "    else:\n",
      "        print \"No Heterodimers found!\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 50
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Test Primers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def search_versions_oligos(oligo, substitution, database):\n",
      "    \"\"\"\n",
      "    Not used\n",
      "    \"\"\"\n",
      "    exp = \"(%s){s<=%d}\" % (oligo, substitution)\n",
      "    def search_oligo(seq, exp = exp):\n",
      "        match = regex.search(exp, seq[1])\n",
      "        if match:\n",
      "            return match.groups()\n",
      "    dview.push({\"search_oligo\":search_oligo,\n",
      "                \"exp\":exp})\n",
      "    task = lview.map(search_oligo, database, chunksize = 500)\n",
      "    wait_on(task)\n",
      "    found = [s for s in task.result if s]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def search_oligos(fwds, revs, substitution, database, return_results = False, return_coverage = False, verbose = True):\n",
      "    \"\"\"\n",
      "    Searches oligos in a list of sequences using regular expression.\n",
      "    Arguments:\n",
      "        -fwds: a list of strings giving the forward primers to be searched.\n",
      "        -revs: a list of strings giving the reverse primers to be searched.\n",
      "        -substitution: an integer giving the maximum number of mismatches allowerd.\n",
      "        -database: a list of strings giving the sequences to be used as template.\n",
      "    Fails if the template and the primers are not in the same orientation.\n",
      "    \"\"\"\n",
      "    fwd_exp = \"|\".join([\"(%s){s<=%d}\"% (primer, substitution) for primer in fwds])\n",
      "    rev_exp = \"|\".join([\"(%s){s<=%d}\"% (primer, substitution) for primer in revs])\n",
      "    fwd_exp = fwd_exp.replace(\"I\", \"[ACTG]\")\n",
      "    rev_exp = rev_exp.replace(\"I\", \"[ACTG]\")\n",
      "    def search_pair(seq, fwd_exp = fwd_exp, rev_exp = rev_exp):\n",
      "        fwd_match = regex.search(fwd_exp, seq[1])\n",
      "        rev_match = regex.search(rev_exp, seq[1])\n",
      "        if fwd_match and rev_match:\n",
      "            return (fwd_match.groups(), rev_match.groups())\n",
      "    dview.push({\"search_pair\":search_pair,\n",
      "                \"fwd_exp\":fwd_exp,\n",
      "                \"rev_exp\":rev_exp})\n",
      "    task = lview.map(search_pair, database, chunksize = 500)\n",
      "    wait_on(task, verbose = verbose)\n",
      "    found = [s for s in task.result if s]\n",
      "    coverage = len(found)\n",
      "    if return_coverage:\n",
      "        return coverage\n",
      "    elif return_results:\n",
      "        return task.result\n",
      "    else:\n",
      "        print \"Coverage is %d out of %d sequences\" % (coverage, len(database))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_mfe_cmd(primers, \n",
      "                 database,\n",
      "                 output,\n",
      "                 mfe_exec,\n",
      "                 ppc,\n",
      "                 min_tm,\n",
      "                 oligo_conc,\n",
      "                 mv_conc,\n",
      "                 dv_conc):\n",
      "    \"\"\"\n",
      "    Make the MFEprimer command to test the primers' coverage and/or specificity.\n",
      "    \"\"\"\n",
      "    cmd = \"%s \" % mfe_exec +\\\n",
      "          \"-i %s \" % primers +\\\n",
      "          \"--oligo_conc=%f \" % oligo_conc +\\\n",
      "          \"-d %s \" % database +\\\n",
      "          \"--mono_conc=%f \" % mv_conc +\\\n",
      "          \"--diva_conc=%f \" % dv_conc +\\\n",
      "          \"--tm_start=%f \" % min_tm +\\\n",
      "          \"--ppc %d \" % ppc +\\\n",
      "          \"--tab \" +\\\n",
      "          \"-o %s \" % output\n",
      "    return cmd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_mfe_parallel(database, \n",
      "                     fwd_list, \n",
      "                     rev_list,\n",
      "                     output,\n",
      "                     min_tm = 40,\n",
      "                     ppc = 30,\n",
      "                     mfe_exec = \"./MFEprimer/MFEprimer.py\",\n",
      "                     oligo_conc = 200,\n",
      "                     mv_conc = 50,\n",
      "                     dv_conc = 1.5):\n",
      "    \"\"\"\n",
      "    Runs MFEprimer in parallel to test the specificity and coverage of a set of primers.\n",
      "    Arguments:\n",
      "        -database: the name of the fasta file where the unaligned sequences are.\n",
      "        -fwd_list: a list of strings giving the fwd primers to be tested.\n",
      "        -rev_list: a list of strings giving the rev primers to be tested.\n",
      "        -output: the name of the output file including the path.\n",
      "        -min_tm: minimal melting temperature for a primer to detected a target sequence.\n",
      "        -ppc: see MFEprimer manual or just keep it as it is.\n",
      "        -mfe_exec: the path for the MFEprimer python file.\n",
      "        -oligo_conc: primer concentration. It should be manually rescaled if degenerate primers\n",
      "                     of multiplex is being used.\n",
      "        -mv_conc: monovalent ions concentration in mM.\n",
      "        -dv_conc: divalent ions concentration in mM.\n",
      "    \"\"\"\n",
      "    # There was a conflict with some other product object, so I decided to import it here\n",
      "    from itertools import product\n",
      "    if os.path.isdir(\"./data/.temp_mfe\"):\n",
      "        shutil.rmtree(\"./data/.temp_mfe\")\n",
      "    os.makedirs(\"./data/.temp_mfe\")\n",
      "    os.makedirs(\"./data/.temp_mfe/primers\")\n",
      "    os.makedirs(\"./data/.temp_mfe/results\")\n",
      "    def write_primer_pair(pair, count_id):\n",
      "        with open(\"./data/.temp_mfe/primers/pair_%d\" % count_id, \"w\") as handle:\n",
      "            handle.write(\">pair_%d_%s_fp\\n%s\\n>pair_%d_%s_rp\\n%s\\n\" % \\\n",
      "                        (count_id, pair[0], pair[0], count_id, pair[1], pair[1]))\n",
      "    primers_pairs = product(fwd_list, rev_list)\n",
      "    pair_dict = {}\n",
      "    for count_id, pair in enumerate(primers_pairs):\n",
      "        pair_dict[\"pair_%d\"%count_id] = pair # Used to index the oligo_conc dict\n",
      "        write_primer_pair(pair, count_id)\n",
      "    primers = os.listdir(\"./data/.temp_mfe/primers/\")\n",
      "    cmds = []\n",
      "    for count, primer in enumerate(primers):\n",
      "        # This is for using a dictionary of oligos concentration\n",
      "        if type(oligo_conc) == dict:\n",
      "            pair = pair_dict[primer]\n",
      "            if pair[0] in oligo_conc:\n",
      "                curr_oligo_conc = oligo_conc[pair[0]]\n",
      "            elif pair[1] in oligo_conc:\n",
      "                curr_oligo_conc = oligo_conc[pair[1]]\n",
      "            else:\n",
      "                raise Exception(\"Oligo concentration invalid! Are the oligos in the correct strand?\")\n",
      "        else:\n",
      "            try:\n",
      "                curr_oligo_conc = float(oligo_conc)\n",
      "            except TypeError:\n",
      "                raise Exception(\"Oligo_conc must be either a number or a dictionary\")\n",
      "        cmd = make_mfe_cmd(primers = \"./data/.temp_mfe/primers/%s\" % primer,\n",
      "             database = database,\n",
      "             output = \"./data/.temp_mfe/results/result_%d\" % count,\n",
      "             min_tm = min_tm,\n",
      "             ppc = ppc,\n",
      "             mfe_exec = mfe_exec,\n",
      "             oligo_conc = curr_oligo_conc,\n",
      "             mv_conc = mv_conc,\n",
      "             dv_conc = dv_conc)\n",
      "        cmds += [cmd]\n",
      "    run_cmd = lambda cmd: os.system(cmd)\n",
      "    dview.push({\"run_cmd\":run_cmd})\n",
      "    task = lview.map(run_cmd, cmds)\n",
      "    wait_on(task)\n",
      "    cmd = (\"cd ./data/.temp_mfe/results/;\"\n",
      "           \"cat $(ls) > all_results;\"\n",
      "           \"awk 'NR==1{print $0} !/AmpID/ {print $0}' all_results > primers_out.txt;\"\n",
      "           \"cd ../../../;\"\n",
      "           \"cp ./data/.temp_mfe/results/primers_out.txt %s\")%output\n",
      "    os.system(cmd)\n",
      "    #shutil.rmtree(\"./data/.temp_mfe\")\n",
      "    return task"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#***************************** In development"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def run_tntblast(nproc, \n",
      "                 fwd_list,\n",
      "                 rev_list,\n",
      "                 output_name, \n",
      "                 database_name, \n",
      "                 min_tm,\n",
      "                 max_tm, \n",
      "                 primer_conc, \n",
      "                 mv_conc,\n",
      "                 dntp_conc = 0.8, \n",
      "                 dv_conc = 1.5,\n",
      "                 plex = False, \n",
      "                 clamp = None, \n",
      "                 rescale_conc = False,\n",
      "                 target_strand =  \"plus\", \n",
      "                 lighter_output = True):\n",
      "    \"\"\"\n",
      "    Tests the primers set using thermonuclotide blast.\n",
      "    Arguments:\n",
      "        -nproc: an integer giving the number of processors to be used by mpi\n",
      "        -query_name: a string with the name of the file containing the primers\n",
      "        -output_name: a string giving the name of the output file\n",
      "        -database_name: a string giving the name of the fasta file to be used as input\n",
      "        -min_tm: an integer with the minimal melting temperature\n",
      "        -max_tm: an integer with the maximal melting temperature\n",
      "        -primer_conc: a string with the primer concentration formatted as float.\n",
      "\n",
      "    \"\"\"\n",
      "    # To avoid conflict with pylab\n",
      "    from itertools import product \n",
      "    pairs = product(fwd_list, rev_list)\n",
      "    with open(\"./data/.tnt_primers\", \"w\") as handle:\n",
      "        for count, pair in enumerate(pairs):\n",
      "            handle.write(\"pair_%d\\t%s\\t%s\\n\" % (count, pair[0], pair[1]))\n",
      "    try:\n",
      "        mv_corrected = mv_conc + 120 * (dv_conc - dntp_conc)**.5\n",
      "    except ValueError:\n",
      "        mv_corrected = mv_conc\n",
      "    cmd = \"mpirun -np %d \"%nproc +\\\n",
      "          \"tntblast -i %s \"% './data/.tnt_primers' +\\\n",
      "          \"-s %fe-3 \"%mv_corrected +\\\n",
      "          \"-o %s \"%output_name +\\\n",
      "          \"-d %s \"%database_name +\\\n",
      "          \"-e %d \"%min_tm +\\\n",
      "          \"-x %d \"%max_tm +\\\n",
      "          \"-t %fe-9 \"%primer_conc +\\\n",
      "          \"--target-strand=%s \"%target_strand\n",
      "    if plex:\n",
      "        cmd += \" --plex=F \"\n",
      "    if clamp:\n",
      "        cmd += \"--primer-clamp=%d \"%clamp\n",
      "    if not rescale_conc:\n",
      "        cmd += \"--rescale-ct=F \"\n",
      "    if lighter_output:\n",
      "        cmd += \" -a F -M F \" \n",
      "    print cmd\n",
      "    process = os.system(cmd)\n",
      "    return process"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#********************* End of development section"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def process_data(fwd_data_name, rev_data_name, delta_tm = 5, return_raw = False):\n",
      "    \"\"\"\n",
      "    Combines the results for fwd and rev primer in a unique dataframe and keeps only the best\n",
      "    match for each sequence detected.\n",
      "    Arguments:\n",
      "        -fwd_data_name: the name of the data file with the results of MFEprimer for foward primers.\n",
      "        -rev_data_name: the name of the data file with the results of MFEprimer for reverse primers.\n",
      "        -delta_tm: maximal allowed absolute difference between fwd and reverse melting temperature in Celsius degrees.\n",
      "    \"\"\"\n",
      "    data_fwd = pandas.read_csv(fwd_data_name, sep = \"\\t\")\n",
      "    data_rev = pandas.read_csv(rev_data_name, sep = \"\\t\")\n",
      "    data_fwd = data_fwd[[\"FpID\", \"HitID\", \"FpTm\", \"BindingStart\"]]\n",
      "    data_rev = data_rev[[\"RpID\", \"HitID\", \"RpTm\", \"BindingStop\"]]\n",
      "    data = pandas.merge(data_rev, data_fwd, on=\"HitID\", how=\"outer\")\n",
      "    data = data.dropna()\n",
      "    data[\"DeltaTm\"] = np.abs(data.FpTm - data.RpTm)\n",
      "    data[\"AmpLen\"] = data.BindingStop - data.BindingStart\n",
      "    data = data.ix[data.DeltaTm <= delta_tm, :]\n",
      "    data[\"fwd_primer\"] = data.FpID.map(lambda x: x.split(\"_\")[2])\n",
      "    data[\"rev_primer\"] = data.RpID.map(lambda x: x.split(\"_\")[2])\n",
      "    data[\"Lowest_Tm\"] = data.apply(lambda row: min(row[\"FpTm\"], row[\"RpTm\"]), axis = 1)\n",
      "    if return_raw:\n",
      "        return data\n",
      "    grouped = data.groupby([\"HitID\"], as_index = False)\n",
      "    data = grouped.apply(lambda group: group.ix[group.Lowest_Tm.idxmax()])\n",
      "    fwds_tm = data.groupby(\"fwd_primer\").FpTm.max()\n",
      "    revs_tm = data.groupby(\"rev_primer\").RpTm.max()\n",
      "    def calculate_max_diff(row, fwds_tm, revs_tm):\n",
      "        fwd = row[\"fwd_primer\"]\n",
      "        rev = row[\"rev_primer\"]\n",
      "        fwd_tm_max = fwds_tm[fwd]\n",
      "        rev_tm_max = revs_tm[rev]\n",
      "        fwd_diff = np.abs(row[\"FpTm\"] - fwd_tm_max)\n",
      "        rev_diff = np.abs(row[\"RpTm\"] - rev_tm_max)\n",
      "        return max(fwd_diff, rev_diff)\n",
      "    data[\"Diff\"] = data.apply(lambda x:calculate_max_diff(x, fwds_tm, revs_tm), axis = 1)\n",
      "    return data"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Add inosine to primers"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calc_tm_general(oligos, \n",
      "                    complements, \n",
      "                    oligo_conc, \n",
      "                    mv_conc, \n",
      "                    dv_conc, \n",
      "                    nn_method = \"all97\",\n",
      "                    salt_method = \"san04\",\n",
      "                    in_file = \"./data/oligos_for_melting\",\n",
      "                    verbose = False):\n",
      "    if not complements:\n",
      "        complements = [str(Seq(oligo).complement()) for oligo in oligos]\n",
      "        complements = [complement.replace(\"I\", \"A\") for complement in complements]\n",
      "    assert type(oligo_conc) in [float, int], \"Oligo concentration must be numeric.\"\n",
      "    assert type(mv_conc) in [float, int], \"Ion concentration must be numeric.\"\n",
      "    assert type(dv_conc) in [float, int], \"Ion concentration must be numeric.\"\n",
      "    with open(in_file, \"w\") as handle:\n",
      "        for oligo, complement in zip(oligos, complements):\n",
      "            handle.write(\"A%sA T%sT\\n\" % (oligo, complement))\n",
      "    melting_path = os.path.abspath(\"./MELTING/executable/melting-batch\") \n",
      "    file_path = os.path.abspath(in_file)\n",
      "    cmd = Template(\n",
      "    \"$melting_path \"\n",
      "    \"-H dnadna \"\n",
      "    \"-nn $nn_method \"\n",
      "    \"-ion $salt_method \"\n",
      "    \"-P ${oligo_conc}e-9 \"\n",
      "    \"-E Na=${mv_conc}e-3:Mg=${dv_conc}e-3 \"\n",
      "    \"$file_path\")\n",
      "    cmd = cmd.substitute(oligo = oligo,\n",
      "                         salt_method = salt_method,\n",
      "                         nn_method = nn_method,\n",
      "                         mv_conc = mv_conc,\n",
      "                         dv_conc = dv_conc,\n",
      "                         oligo_conc = oligo_conc,\n",
      "                         melting_path = melting_path,\n",
      "                         file_path = file_path)\n",
      "    if verbose:\n",
      "        print cmd\n",
      "    os.system(cmd)\n",
      "    out_file = in_file + \".results.csv\"\n",
      "    # To remove a ^M character that is preventing the file to be read\n",
      "    cmd = \"awk '!/Delta/{print $1, $2, $3, $4, $5}' %s > %s\" % \\\n",
      "          (out_file, out_file + \".modified\")\n",
      "    os.system(cmd)\n",
      "    results = pandas.read_table(out_file + \".modified\", sep = \"[\\s\\t]\", header = None)\n",
      "    results.columns = [\"Oligo\", \"Match\", \"DeltaH\",\n",
      "                       \"DeltaS\", \"Tm\"]\n",
      "    #os.remove(in_file)\n",
      "    #os.remove(out_file)\n",
      "    #os.remove(out_file + \".modified\")\n",
      "    #results[\"Oligo\"] = results[\"Oligo\"].map(lambda x: x[1:-1])\n",
      "    #results[\"Match\"] = results[\"Match\"].map(lambda x: x[1:-1])\n",
      "    # To make it comparable with the calculations using primer3\n",
      "    results[\"Tm\"] = results[\"Tm\"] - 1\n",
      "    return results[[\"Oligo\", \"Match\", \"Tm\"]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_match_mfe(data, HitID, oligo, binding, rev = False):\n",
      "    if rev:\n",
      "        start = binding - len(oligo)\n",
      "        stop = binding\n",
      "    else:\n",
      "        start = binding -1\n",
      "        stop = binding + len(oligo) - 1\n",
      "    match = data[HitID][int(start):int(stop)]\n",
      "    if rev:\n",
      "        match = str(match[::-1])\n",
      "    else:\n",
      "        match = str(Seq(match).complement())\n",
      "    return match"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_all_matches_from_mfe(data, mfe_database, fwd_dummy, min_tm):\n",
      "    records = make_dict_records(\"./data/mfe/Nitrososphaera_dummy.genomic\")\n",
      "    records = {key:str(value.seq) for key, value in records.iteritems()}\n",
      "    msg = (\"Database does not contain dummy oligos. Please provide the same database\"\n",
      "       \"used in run_mfe_parallel function.\")\n",
      "    assert fwd_dummy in records.values()[0], msg\n",
      "    data_valid = data.ix[(data.FpTm > min_tm) & (data.RpTm > min_tm), :]\n",
      "    for oligo, binding, match in [(\"fwd_primer\", \"BindingStart\", \"MatchFwd\"),\n",
      "                                  (\"rev_primer\", \"BindingStop\", \"MatchRev\")]:\n",
      "        unique = set(zip(data_valid[\"HitID\"], data_valid[oligo], data_valid[binding]))\n",
      "        unique = list(unique)\n",
      "        rev = oligo == \"rev_primer\"\n",
      "        matches = map(lambda x: find_match_mfe(data = records, \n",
      "                                               HitID = x[0], \n",
      "                                               oligo = x[1], \n",
      "                                               binding = x[2],\n",
      "                                               rev = rev),\n",
      "                      unique)\n",
      "        data_unique = pandas.DataFrame(unique, columns = [\"HitID\", oligo, binding])\n",
      "        data_unique[match] = matches\n",
      "        data_valid = pandas.merge(data_valid, data_unique, on = [\"HitID\", oligo, binding])\n",
      "    return data_valid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def correct_tm(data, total_oligo_conc, mv_conc = 50, dv_conc = 1.5, min_tm = 40, verbose = False):\n",
      "    data_valid = data.ix[(data.FpTm > min_tm) & (data.RpTm > min_tm), :]\n",
      "    for column_primer, column_match in [(\"fwd_primer\", \"MatchFwd\"), \n",
      "                                        (\"rev_primer\", \"MatchRev\")]:\n",
      "        unique_comb = set(zip(data_valid[column_primer], data_valid[column_match]))\n",
      "        oligos = [unique[0] for unique in unique_comb]\n",
      "        complements = [unique[1] for unique in unique_comb]\n",
      "        oligo_conc_rescaled = total_oligo_conc / float(len(set(oligos)))\n",
      "        tm = calc_tm_general(oligos = oligos,\n",
      "                             complements = complements,\n",
      "                             oligo_conc = oligo_conc_rescaled,\n",
      "                             mv_conc = mv_conc,\n",
      "                             dv_conc = dv_conc,\n",
      "                             verbose = verbose)\n",
      "        rename_dict = {\"Oligo\":column_primer, \n",
      "                       \"Match\":column_match, \n",
      "                       \"Tm\":(\"tm_\" + column_primer[:3])}\n",
      "        tm = tm.rename(columns = rename_dict)\n",
      "        data_valid = pandas.merge(data_valid, tm, on = [column_primer, column_match])\n",
      "    return data_valid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_low_agreement_pos(seqs, min_agreement):\n",
      "    \"\"\"\n",
      "    Find positions of most frequent mismatches in a given primer.\n",
      "    It is used in the function add_inosine.\n",
      "    \"\"\"\n",
      "    pos = zip(*seqs)\n",
      "    comp = {}\n",
      "    for count, p in enumerate(pos):\n",
      "        comp_i = pandas.Series(p).value_counts() / len(p)\n",
      "        max_agreement = comp_i.max()\n",
      "        comp[count] = max_agreement\n",
      "    comp = pandas.Series(comp)\n",
      "    comp = comp[comp < min_agreement]\n",
      "    return comp"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filter_low_agreement_positions(pos, oligo, allowed_bases, \n",
      "                                   min_distance, max_inos_add):\n",
      "    \"\"\"\n",
      "    Discard regions of high variability in the primer if they are not suitable for inosine addition.\n",
      "    It is used in the function add_inosine.\n",
      "    \"\"\"\n",
      "    pos.sort()\n",
      "    # Discard positions whose base is not allowed to be substituted by inosine\n",
      "    good_pos = [p for p in pos.index if oligo[p] in allowed_bases]\n",
      "    included_pos = []\n",
      "    for count, p in enumerate(good_pos):\n",
      "        if count == 0:\n",
      "            included_pos += [p]\n",
      "            continue\n",
      "        distances = [np.abs(p - p_inc) for p_inc in included_pos]\n",
      "        close = [d for d in distances if d < min_distance]\n",
      "        if close:\n",
      "            continue\n",
      "        else:\n",
      "            included_pos += [p]\n",
      "        if len(included_pos) >= max_inos_add:\n",
      "            break\n",
      "    return included_pos"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_inosine(oligos, records, min_agreement = .85, allowed_bases = \"ATG\", \n",
      "                min_distance = 5, max_inos_add = 3):\n",
      "    \"\"\"\n",
      "    Adds inosine to the primers.\n",
      "    Arguments:\n",
      "        -oligos: a list of primers where the inosine should be added.\n",
      "        -records: a list of records as returned by the function read_fasta.\n",
      "        -min_agreement: float. Highest proportion allowed of the most abundant base in a given position \n",
      "                        for making it a candidate to inosine addition.\n",
      "        -allowed_bases: string giving the bases that can be substituted by inosine in the original primer.\n",
      "        -min_distance: minimal spacing in number of bases that should be between inosines.\n",
      "        -max_inos_add: maximal number of inosines per primer.\n",
      "    Currently there is no way to test for heterodimers, homodimers of hairpins in primers with inosine using\n",
      "    primer3, so the user should test if the inosine addition is causing this kind of problem using tools\n",
      "    like bioanalyzer.\n",
      "    \"\"\"\n",
      "    oligos_degen = []\n",
      "    for oligo in oligos:\n",
      "        exp = \"(%s){s<=3}\"%oligo\n",
      "        seqs = [regex.findall(exp, str(record.seq)) for record in records]\n",
      "        seqs = [s[0] for s in seqs if s]\n",
      "        low_agree = find_low_agreement_pos(seqs, min_agreement)\n",
      "        included_pos = filter_low_agreement_positions(pos = low_agree, \n",
      "                                                      oligo = oligo, \n",
      "                                                      allowed_bases = allowed_bases, \n",
      "                                                      min_distance = min_distance,\n",
      "                                                      max_inos_add = max_inos_add)\n",
      "        oligo = list(oligo)\n",
      "        if included_pos:\n",
      "            for p in included_pos:\n",
      "              oligo[p] = \"I\"  \n",
      "        oligos_degen += [\"\".join(oligo)]\n",
      "    return oligos_degen"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_redundant_after_inosine_addition(data, min_increase):\n",
      "    \"\"\"\n",
      "    Removes oligos that don't increase coverage after inosine addition.\n",
      "    Arguments:\n",
      "        -data: a pandas dataframe with sequences detected by the oligo as returned by measure_coverage_oligos\n",
      "        -min_increase: minimal increase of coverage for a oligo to be kept.\n",
      "    \"\"\"\n",
      "    cover = data.sum().sort(inplace = False, ascending = False)\n",
      "    to_del = []\n",
      "    for primer in cover.index:\n",
      "        if primer == cover.index[0]:\n",
      "            previously_detected = data[primer]\n",
      "            continue\n",
      "        union = data[primer] | previously_detected\n",
      "        increase = union.sum() - previously_detected.sum()\n",
      "        if increase < min_increase:\n",
      "            to_del += [primer]\n",
      "    return to_del"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def measure_coverage_oligo(oligos, database, substitution, rev = False):\n",
      "    \"\"\"\n",
      "    Measure the coverage of oligos. Used to discard redundancy.\n",
      "    Arguments:\n",
      "        -oligos: a list of oligos to search.\n",
      "        -database: the template sequences to search.\n",
      "        -substitution: maximal number of substitutions (I highly recommend to keep it as 0).\n",
      "        -rev: is the oligo a reverse primer?\n",
      "    \"\"\"\n",
      "    covers = {oligo:None for oligo in oligos}\n",
      "    for oligo in oligos:\n",
      "        if rev:\n",
      "            fwds = [\"I\"]\n",
      "            revs = [oligo]\n",
      "        else:\n",
      "            fwds = [oligo]\n",
      "            revs = [\"I\"]\n",
      "        covers[oligo] = search_oligos(fwds = fwds, \n",
      "                                    revs = revs,\n",
      "                                    substitution = substitution, \n",
      "                                    database = database, \n",
      "                                    return_results = True)\n",
      "    covers = pandas.DataFrame(covers)\n",
      "    covers = covers.notnull().astype(int)\n",
      "    return covers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def distribute_conc_by_cover(data, total_conc):\n",
      "    \"\"\"\n",
      "    Divides the total concentration of the primers proportionally to the number of sequences detected by each primer.\n",
      "    Arguments:\n",
      "        -data: a pandas dataframe with sequences detected by the oligo as returned by remove_redundant_after_inosine_addition\n",
      "        -total_conc: total primer concentration in nM\n",
      "    \"\"\"\n",
      "    detected = data.sum(axis = 1) > 0\n",
      "    data = data.ix[detected, :]\n",
      "    effec_conc = 200 / float(len(data.index))\n",
      "    ind_conc = (float(1) / data.sum(axis = 1)) * effec_conc\n",
      "    conc = data.apply(lambda x: x*ind_conc, axis = 0).sum()\n",
      "    return dict(conc)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_complement_conc_rev(conc_rev):\n",
      "    new_dict = {}\n",
      "    for primer, conc in conc_rev.iteritems():\n",
      "        comp = str(Seq(primer).reverse_complement())\n",
      "        new_dict[comp] = conc\n",
      "    return new_dict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def prune_inosine(fwd_degen_list,\n",
      "                  fwd_degen_dict,\n",
      "                  rev_degen_list,\n",
      "                  rev_degen_dict,\n",
      "                  orig_coverage,\n",
      "                  min_increase,\n",
      "                  rev = False):\n",
      "    \"\"\"\n",
      "    Removes inosines that don't increase coverage in relation to sequences already detected by other primers.\n",
      "    Arguments:\n",
      "        -fwd_degen_list: a list of forward degenerate primers\n",
      "        -rev_degen_list: a list of reverse degenerate primers\n",
      "        -fwd_degen_dict: a dictionary of forward degenerate primers\n",
      "        -rev_degen_dict: a dictionary of reverse degenerate primers\n",
      "        -orig_coverage: initial coverage.\n",
      "        -min_increase: minimal decrease in number of sequences detected to keep a inosine.\n",
      "        -rev: is the oligo a reverse primer?\n",
      "    \"\"\"\n",
      "    if rev:\n",
      "        oligo_list = rev_degen_list\n",
      "        oligo_dict = rev_degen_dict\n",
      "    else:\n",
      "        oligo_list = fwd_degen_list\n",
      "        oligo_dict = fwd_degen_dict\n",
      "    for pos, oligo in enumerate(oligo_list):\n",
      "        inosines = [p for p, b in enumerate(oligo) if b == \"I\"]\n",
      "        orig_oligo = oligo_dict[oligo]\n",
      "        for inos_pos in inosines:\n",
      "            oligo_wo_1_inos = list(oligo)\n",
      "            oligo_wo_1_inos[inos_pos] = orig_oligo[inos_pos]\n",
      "            oligo_wo_1_inos = \"\".join(oligo_wo_1_inos)\n",
      "            oligo_list[pos] = oligo_wo_1_inos\n",
      "            if rev:\n",
      "                rev_list_for_test = oligo_list\n",
      "                fwd_list_for_test = fwd_degen_list\n",
      "            else:\n",
      "                rev_list_for_test = rev_degen_list\n",
      "                fwd_list_for_test = oligo_list\n",
      "            mod_coverage = search_oligos(fwds = fwd_list_for_test, \n",
      "                                         revs = rev_list_for_test, \n",
      "                                         substitution = 0, \n",
      "                                         database = database,\n",
      "                                         return_coverage = True, \n",
      "                                         verbose = False)\n",
      "            if orig_coverage - mod_coverage >= min_increase:\n",
      "                oligo_list[pos] = oligo # Return to previous value\n",
      "            else:\n",
      "                oligo = oligo_list[pos] # Update the reference for next iterations\n",
      "                orig_coverage = mod_coverage\n",
      "        clear_output()\n",
      "        msg_oligo = \"reverse\" if rev else \"forward\"\n",
      "        print \"Prunning %s primer %d of %d.\" % (msg_oligo, pos+1, len(oligo_list))\n",
      "        sys.stdout.flush()\n",
      "    return oligo_list"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def prune_inosine_fwd_rev(rev_degen_list,\n",
      "                          fwd_degen_list,\n",
      "                          fwd_degen_dict,\n",
      "                          rev_degen_dict,\n",
      "                          database,\n",
      "                          min_increase = 20):\n",
      "    \"\"\"\n",
      "    Applies the function prune_inosine to remove inosines that don't increase coverage in relation to \n",
      "    sequences already detected by other primers.\n",
      "    Arguments:\n",
      "        -fwd_degen_list: a list of forward degenerate primers\n",
      "        -rev_degen_list: a list of reverse degenerate primers\n",
      "        -fwd_degen_dict: a dictionary of forward degenerate primers\n",
      "        -rev_degen_dict: a dictionary of reverse degenerate primers\n",
      "        -database: a list of sequences to be searched against the oligos.\n",
      "        -min_increase: minimal decrease in number of sequences detected to keep a inosine.\n",
      "    \"\"\"\n",
      "    orig_coverage = search_oligos(fwds = fwd_degen_list, \n",
      "                                  revs = rev_degen_list, \n",
      "                                  substitution = 0, database = database,\n",
      "                                  return_coverage = True,\n",
      "                                  verbose = False)\n",
      "    fwd_degen_list = prune_inosine(fwd_degen_list = fwd_degen_list,\n",
      "                                   fwd_degen_dict = fwd_degen_dict,\n",
      "                                   rev_degen_list = rev_degen_list,\n",
      "                                   rev_degen_dict = rev_degen_dict,\n",
      "                                   orig_coverage = orig_coverage,\n",
      "                                   min_increase = min_increase,\n",
      "                                   rev = False)\n",
      "    orig_coverage = search_oligos(fwds = fwd_degen_list, \n",
      "                                  revs = rev_degen_list, \n",
      "                                  substitution = 0, database = database,\n",
      "                                  return_coverage = True,\n",
      "                                  verbose = False)\n",
      "    rev_degen_list = prune_inosine(fwd_degen_list = fwd_degen_list,\n",
      "                                   fwd_degen_dict = fwd_degen_dict,\n",
      "                                   rev_degen_list = rev_degen_list,\n",
      "                                   rev_degen_dict = rev_degen_dict,\n",
      "                                   orig_coverage = orig_coverage,\n",
      "                                   min_increase = min_increase,\n",
      "                                   rev = True)\n",
      "    return {\"fwd_degen\":fwd_degen_list, \n",
      "            \"rev_degen\":rev_degen_list}\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def classify_seqs(HitID, records_class):\n",
      "    try:\n",
      "        seq_class = records_class[HitID]\n",
      "    except KeyError:\n",
      "        seq_class = \"unknown\"\n",
      "    return seq_class"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Discard redundant oligos"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_redundant_one(data_raw_filtered, min_increase, rev = False):\n",
      "    \"\"\"\n",
      "    Removes sequences that are redundant based on thermodynamic simulations.\n",
      "    Arguments:\n",
      "        -data_raw_filtered: raw data from a MFEprimer run.\n",
      "        -min_increase: minimal increase in coverage to keep a primer.\n",
      "        -rev: is the oligo a reverse primer?\n",
      "    \"\"\"\n",
      "    primer_pos = \"rev_primer\" if rev else \"fwd_primer\"\n",
      "    grouped = data_raw_filtered.groupby([primer_pos])\n",
      "    covered = grouped.apply(lambda x: set(x[\"HitID\"].unique()))\n",
      "    coverage = covered.map(len).sort(inplace = False)\n",
      "    for primer in coverage.index:\n",
      "        original_coverage = len(data_raw_filtered.HitID.unique())\n",
      "        data_wo_one = data_raw_filtered.ix[data_raw_filtered[primer_pos] != primer, :]\n",
      "        mod_coverage = len(data_wo_one.HitID.unique())\n",
      "        if (original_coverage - mod_coverage) <= min_increase:\n",
      "            data_raw_filtered = data_wo_one\n",
      "    return data_raw_filtered"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_redundant_all(data_raw, min_increase):\n",
      "    \"\"\"\n",
      "    Removes sequences that are redundant based on thermodynamic simulations.\n",
      "    Arguments:\n",
      "        -data_raw: raw data from MFEprimer (see example)\n",
      "        -min_increase: minimal increase in coverage to keep a primer.\n",
      "    \"\"\"\n",
      "    min_increase = 60\n",
      "    grouped = data_raw.groupby([\"fwd_primer\", \"rev_primer\"])\n",
      "    grouped_fwd = data_raw.groupby([\"fwd_primer\"])\n",
      "    grouped_rev = data_raw.groupby([\"rev_primer\"])\n",
      "    mean_fwd = grouped_fwd.FpTm.max().mean()\n",
      "    mean_rev = grouped_rev.RpTm.max().mean()\n",
      "    c1 = data_raw.FpTm.map(lambda x: numpy.abs(x - mean_fwd) <= 4) # Is Tm 4C above or below median Tm?\n",
      "    c2 = data_raw.RpTm.map(lambda x: numpy.abs(x - mean_rev) <= 4)\n",
      "    c3 = data_raw.DeltaTm <= 5 # Is delta Tm > 5?\n",
      "    data_raw_filtered = data_raw.ix[c1 & c2 & c3, :]\n",
      "    data_filtered = remove_redundant_one(data_raw_filtered, min_increase, rev = False)\n",
      "    data_filtered = remove_redundant_one(data_filtered, min_increase, rev = True)\n",
      "    return data_filtered"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def substitute_inosine(oligo):\n",
      "    iupac = {\n",
      "    \"A\":\"A\",\n",
      "    \"C\":\"C\",\n",
      "    \"T\":\"T\",\n",
      "    \"G\":\"G\",\n",
      "    \"R\":\"AG\",\n",
      "    \"Y\":\"CT\",\n",
      "    \"S\":\"GC\",\n",
      "    \"W\":\"AT\",\n",
      "    \"K\":\"GT\",\n",
      "    \"M\":\"AC\",\n",
      "    \"B\":\"CGT\",\n",
      "    \"D\":\"AGT\",\n",
      "    \"H\":\"ACT\",\n",
      "    \"V\":\"ACG\",\n",
      "    \"N\":\"ACTG\",\n",
      "    \"I\":\"ACTG\",\n",
      "    }\n",
      "    oligo = list(oligo)\n",
      "    sub_primers = [iupac[base] for base in oligo]\n",
      "    sub_primers = list(product(*sub_primers))\n",
      "    sub_primers = [\"\".join(sub_primer) for sub_primer in sub_primers]\n",
      "    return sub_primers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_degenerate(oligos):\n",
      "    iupac = { \n",
      "     'A': 'A',\n",
      "     'AC': 'M',\n",
      "     'ACG': 'V',\n",
      "     'ACT': 'H',\n",
      "     'ACTG': 'N',\n",
      "     'AG': 'R',\n",
      "     'AGT': 'D',\n",
      "     'AT': 'W',\n",
      "     'C': 'C',\n",
      "     'CGT': 'B',\n",
      "     'CT': 'Y',\n",
      "     'G': 'G',\n",
      "     'GC': 'S',\n",
      "     'GT': 'K',\n",
      "     'T': 'T'\n",
      "     }\n",
      "    for key in iupac.keys():\n",
      "        permuted_keys = permutations(key)\n",
      "        for perm_key in permuted_keys:\n",
      "            iupac[\"\".join(perm_key)] = iupac[key]\n",
      "    bases = [set(column) for column in zip(*oligos)]\n",
      "    bases = [\"\".join(pos) for pos in bases]\n",
      "    print \"|\".join(bases)\n",
      "    bases = [iupac[pos] for pos in bases]\n",
      "    return \"\".join(bases)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def verify_dimers_degen_primers(fwd_primer_list, \n",
      "                                rev_primer_list, \n",
      "                                oligo_conc,\n",
      "                                mv_conc = 50, \n",
      "                                dv_conc = 1.5,\n",
      "                                max_dimer_tm = 35):\n",
      "    # Use highest concentration to calculate Tm (conservative)\n",
      "    \n",
      "    oligos = fwd_primer_list + rev_primer_list\n",
      "    expanded_oligos = []\n",
      "    for oligo in oligos:\n",
      "        expanded_oligos += substitute_inosine(oligo)\n",
      "    comb_oligos = combinations(expanded_oligos, 2)\n",
      "    found = False\n",
      "    for pair in comb_oligos:\n",
      "        tm = primer3.calcHeterodimerTm(pair[0], \n",
      "                                       pair[1],\n",
      "                                       dna_conc = oligo_conc,\n",
      "                                       dv_conc = dv_conc,\n",
      "                                       mv_conc = mv_conc)\n",
      "        if tm > max_dimer_tm:\n",
      "            print \"Dimer found!\"\n",
      "            found = True\n",
      "            print pair\n",
      "            print \"Melting temperature: %f\" % tm\n",
      "    if not found:\n",
      "        print \"No dimer found.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def verify_hairpins(oligo_list,\n",
      "                    oligo_conc,\n",
      "                    max_hairpin_tm = 35, \n",
      "                    dv_conc = 1.5, \n",
      "                    mv_conc = 1.5):\n",
      "    found = False\n",
      "    for oligo in oligo_list:\n",
      "        tm = primer3.calcHairpinTm(oligo, \n",
      "                                   dna_conc = oligo_conc,\n",
      "                                   dv_conc = dv_conc,\n",
      "                                   mv_conc = mv_conc)\n",
      "        if tm > max_hairpin_tm:\n",
      "            print \"Hairpin found!\"\n",
      "            found = True\n",
      "            print oligo\n",
      "            print \"Melting temperature: %f\" % tm\n",
      "    if not found:\n",
      "        print \"No hairpin found.\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Tools for alignment visualization"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def deduplicate_for_visualization(aligned_fasta_file):\n",
      "    print \"Removing duplicated sequences.\"\n",
      "    cluster_sequences(input_file = \"./data/ordered/unaligned.fasta\",\n",
      "                      output_file = \"./data/ordered/unaligned_not_amb.fasta\",\n",
      "                      similarity = 1,\n",
      "                      word_size = 10)\n",
      "    not_amb = make_dict_records(\"./data/ordered/unaligned_not_amb.fasta\")\n",
      "    aligned_amb = make_dict_records(aligned_fasta_file)\n",
      "    aligned_not_amb = [aligned_amb[seq_id] for seq_id in not_amb.keys()]\n",
      "    write_fasta(\"./data/ordered/aligned_not_amb.fasta\", aligned_not_amb)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def find_start_end(seq1, seq2):\n",
      "    start_1 = regex.search(r\"^[-\\.]*\",  seq1)\n",
      "    start_2 = regex.search(r\"^[-\\.]*\",  seq2)\n",
      "    end_1 = regex.search(r\"[-\\.]*$\",  seq1)\n",
      "    end_2 = regex.search(r\"[-\\.]*$\",  seq2)\n",
      "    start = max(start_1.end(), start_2.end())\n",
      "    end = min(end_1.start(), end_2.start())\n",
      "    return (start, end)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calculate_distance(seq1, seq2):\n",
      "    start, end = find_start_end(seq1, seq2)\n",
      "    distance = sum(a != b for a,b in zip(seq1[start:end], seq2[start:end])) / float(end - start)\n",
      "    return distance"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def write_ordered_fasta(ref_id,\n",
      "                        out_fasta, similarity_threshold,\n",
      "                        original_aligned_file,\n",
      "                        fasta_file = \"./data/ordered/aligned_not_amb.fasta\"):\n",
      "    records = make_dict_records(fasta_file)\n",
      "    # The ref_id could be removed after deduplication.\n",
      "    if ref_id in records.keys():\n",
      "        ref_seq = str(records[ref_id].seq)\n",
      "    else:\n",
      "        original_records = make_dict_records(original_aligned_file)\n",
      "        try:\n",
      "            ref_seq = str(original_records[ref_id].seq)\n",
      "        except KeyError:\n",
      "            raise Exception(\"%s is not in the provided fasta file!\" % ref_seq)\n",
      "    targets = [str(record.seq) for record in records.values()]\n",
      "    dists = map(lambda target: calculate_distance(ref_seq, target), targets)\n",
      "    dists = {seq_id:dist for seq_id, dist in zip(records.keys(), dists)}\n",
      "    dists = pandas.Series(dists)\n",
      "    dists.sort(inplace = True)\n",
      "    dists = dists[dists <= (1 - similarity_threshold)]\n",
      "    records_sorted = []\n",
      "    for count, seq_id in enumerate(dists.index):\n",
      "        records_sorted += [records[seq_id]]\n",
      "        records_sorted[count].id += \"_%.2f%%\"% ((1 - dists[seq_id]) * 100)\n",
      "    write_fasta(out_fasta, records_sorted)\n",
      "    return records_sorted"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calculate_composition(records_ordered, reference_seq_id):\n",
      "    print \"Calculating composition.\"\n",
      "    seqs = [str(record.seq) for record in records_ordered]\n",
      "    pos = zip(*seqs)\n",
      "    compositions = []\n",
      "    for column in pos:\n",
      "        composition = {\"A\":0, \"C\":0, \"T\":0, \"G\":0, \"Others\":0}\n",
      "        for base in column:\n",
      "            try:\n",
      "                composition[base.upper()] += 1\n",
      "            except KeyError:\n",
      "                composition[\"Others\"] += 1\n",
      "        compositions += [composition]\n",
      "    data = pandas.DataFrame(compositions) / len(seqs)\n",
      "    data[\"Seq_ID\"] = reference_seq_id\n",
      "    data = data.reset_index()\n",
      "    if not os.path.isdir(\"./data/ordered/compositions\"):\n",
      "        os.makedirs(\"./data/ordered/compositions\")\n",
      "    data.to_csv(\"./data/ordered/compositions/%s.csv\" % reference_seq_id)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def concatenate_compositions():\n",
      "    files = os.listdir(\"./data/ordered/compositions/\")\n",
      "    cur_file = files.pop()\n",
      "    data = pandas.read_csv(\"./data/ordered/compositions/%s\" % cur_file)\n",
      "    while files:\n",
      "        cur_file = files.pop()\n",
      "        cur_data = pandas.read_csv(\"./data/ordered/compositions/%s\" % cur_file)\n",
      "        data = pandas.concat([data, cur_data])\n",
      "    data.to_csv(\"./data/ordered/compositions.csv\", index = False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def main_visualization(reference_seq_ids, \n",
      "                       aligned_fasta_file, \n",
      "                       out_fasta, \n",
      "                       similarity_threshold,\n",
      "                       composition_name,\n",
      "                       classes = None,\n",
      "                       deduplicate = True):\n",
      "    \"\"\"\n",
      "    This function reorders the provided aligned sequences in a fasta file according to their \n",
      "    similarity to a reference sequence in the file. It also plot the base composition for the \n",
      "    resulting ordered fasta file.\n",
      "    Arguments:\n",
      "        -reference_seq_ids: must be a list with one or more references. Note the reference is the \n",
      "                            first \"word\" that follows tha > sign in a fasta file. For example, a\n",
      "                            record where the sequence description is as follow:\n",
      "                            >Bradyrhizobium_3 japonicum\n",
      "                            The reference for this sequence would be \"Bradyrhizobium_3\".\n",
      "                            Also note that this argument must be a list, meaning that the reference(s)\n",
      "                            must be enclosed by square brackets [] as in the example.\n",
      "        -aligned_fasta_file: a string giving the name (including the path) of the aligned fasta file\n",
      "                             to be ordered. This function takes only one fasta file at a time.\n",
      "        -out_fasta: the prefix of the name of the output file. The reference sequence ID will be added\n",
      "                    to this prefix to make the name of the file. Make sure you include the path.\n",
      "        -similarity_threshold: them minimum similarity to the reference. Sequences will be discarded \n",
      "                               if they are less similar then the threshold. The distance used is the\n",
      "                               Hamming distance proportional to sequence size. Pairwise deletion is \n",
      "                               used to deal with missing data.\n",
      "        -composition_name: the name of the pdf file with the composition plot, including the path.\n",
      "        -classes: an optional argument in case you want to give a meaningful title for each composition\n",
      "                  plot instead of the reference_id. Note that it must be a list and the order of the \n",
      "                  elements is correspondent to the order of the elements in reference_seq_ids.\n",
      "        -deduplicate: a boolean (True or False) argument indicating whether or not the fasta file\n",
      "                      should be deduplicated.\n",
      "    \"\"\"\n",
      "    if not classes:\n",
      "        classes = reference_seq_ids\n",
      "    if type(reference_seq_ids) != list:\n",
      "        raise Exception(\"reference_seq_ids must be given as a list!\")\n",
      "    if os.path.isdir(\"./data/ordered\"):\n",
      "        shutil.rmtree(\"./data/ordered\")\n",
      "    records_test = make_dict_records(aligned_fasta_file)\n",
      "    for seq_id in reference_seq_ids:\n",
      "        assert seq_id in records_test.keys(), \"%s not in the provided fasta file!\" % seq_id\n",
      "    os.makedirs(\"./data/ordered\")\n",
      "    dealign_seq(in_file_name = aligned_fasta_file, \n",
      "                out_file_name = \"./data/ordered/unaligned.fasta\")\n",
      "    # Create the \"./data/ordered/aligned_not_amb.fasta\"\n",
      "    if deduplicate:\n",
      "        deduplicate_for_visualization(aligned_fasta_file = aligned_fasta_file)\n",
      "    else:\n",
      "        shutil.copyfile(aligned_fasta_file, \n",
      "                        \"./data/ordered/aligned_not_amb.fasta\")\n",
      "    out_aligned_files = []\n",
      "    for pos, reference_seq_id in enumerate(reference_seq_ids):\n",
      "        records_ordered = write_ordered_fasta(ref_id = reference_seq_id,\n",
      "                                              out_fasta = out_fasta + reference_seq_id, \n",
      "                                              original_aligned_file = aligned_fasta_file,\n",
      "                                              similarity_threshold = similarity_threshold)\n",
      "        calculate_composition(records_ordered, classes[pos])\n",
      "        out_aligned_files += [out_fasta + reference_seq_id]\n",
      "    concatenate_compositions()\n",
      "    if not \".pdf\"  in composition_name:\n",
      "        composition_name = composition_name + \".pdf\"\n",
      "    make_r_script(composition_name)\n",
      "    process = os.system(\"Rscript ./data/ordered/plot_composition.R\")\n",
      "    if process:\n",
      "        print \"It was not possible to plot the composition.\"\n",
      "    else:\n",
      "        shutil.rmtree(\"./data/ordered\")\n",
      "        print \"Done!\"\n",
      "        print \"The pdf file with compositions was saved as: %s.\" % composition_name\n",
      "        print \"The ordered fasta file(s) were/was saved as follows:\"\n",
      "        for file_out in out_aligned_files:\n",
      "            print file_out"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def make_r_script(composition_name):\n",
      "    script = (\"\"\"\n",
      "    library(plyr)\n",
      "    library(reshape2)\n",
      "    library(ggplot2)\n",
      "\n",
      "    orig_data <- read.csv(\"./data/ordered/compositions.csv\", stringsAsFactors = FALSE)\n",
      "\n",
      "    data <- melt(orig_data[, -1], \n",
      "                 id.vars = c(\"index\", \"Seq_ID\"), \n",
      "                 value.name = \"Freq\",\n",
      "                 variable.name = \"Base\")\n",
      "\n",
      "    data$Freq <- as.numeric(data$Freq)\n",
      "\n",
      "    width = length(unique(data$index)) / 8\n",
      "    max_index = max(data$index)\n",
      "\n",
      "    data = ddply(data, c(\"index\", \"Seq_ID\"), transform, Order = order(Freq, decreasing = TRUE))\n",
      "    data = ddply(data, c(\"index\", \"Seq_ID\"), function(x) x[x$Order, ])\n",
      "    data = ddply(data, c(\"index\", \"Seq_ID\"), transform, FreqSum = cumsum(Freq))\n",
      "    data$Base <- as.character(data$Base)\n",
      "    data$Base[data$Base == \"Others\"] <- \"-\"\n",
      "    data$Seq_ID <- as.factor(data$Seq_ID)\n",
      "    add_space = function(x, n) paste(rep(paste(rep(\" \", 100), collapse = \"\"), n), x, collapse = \"\")\n",
      "    n = max(data$index) / 30\n",
      "    for(level in levels(data$Seq_ID)){\n",
      "      levels(data$Seq_ID)[levels(data$Seq_ID) == level] <- add_space(level, n)\n",
      "    }\n",
      "    \n",
      "    \n",
      "    theme_set(theme_minimal(9))\n",
      "    graph = ggplot(data) +\n",
      "      aes(x = index, fill = reorder(Base, Freq), y = Freq) +\n",
      "      facet_wrap(~ Seq_ID, ncol = 1) +\n",
      "      geom_bar(stat = \"identity\", \n",
      "               colour = \"black\", \n",
      "               size = .2, \n",
      "               show_guide = FALSE,\n",
      "               width = 1) +\n",
      "      scale_fill_manual(values = c(\"C\" = \"red\", \n",
      "                                   \"A\" = \"blue\", \n",
      "                                   \"T\" = \"green\" , \n",
      "                                   \"-\" = \"grey\", \n",
      "                                   \"G\" = \"orange\")) +\n",
      "      geom_text(aes(y = FreqSum - Freq / 2, \n",
      "                    label = Base,\n",
      "                    size = Freq), \n",
      "                show_guide = FALSE) +\n",
      "      scale_size_continuous(range = c(0.5, 4)) +\n",
      "      labs(x = \"Position\", \n",
      "           y = \"Frequency\", \n",
      "           fill = \"Bases\") +\n",
      "      scale_x_continuous(expand = c(0, 0), \n",
      "                         breaks = seq(0, max_index),\n",
      "                         labels = seq(1, max_index + 1)) +\n",
      "      theme(axis.text.x = element_text(angle = 90, hjust = 1),\n",
      "            legend.position = \"left\",\n",
      "            strip.text.x = element_text(size=8))\n",
      "\n",
      "\n",
      "    ggsave(\"%s\", width = width, height = 1.5 * length(unique(data$Seq_ID)), limitsize = FALSE)\n",
      "    \"\"\") % composition_name\n",
      "    with open(\"./data/ordered/plot_composition.R\", \"w\") as handle:\n",
      "        handle.write(script)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def create_test_train_datasets(original_data_algn, original_data_not_algn, prop_test = .3):\n",
      "    records_algn = make_dict_records(original_data_algn)\n",
      "    records_not_algn = make_dict_records(original_data_not_algn)\n",
      "    n_seq = len(records_algn)\n",
      "    test_size = int(prop_test * n_seq)\n",
      "    sample_test = random.sample(xrange(n_seq), test_size)\n",
      "    sample_train = set(xrange(n_seq)) - set(sample_test)\n",
      "    acc_test = [records_algn.keys()[i] for i in sample_test]\n",
      "    acc_train = [records_algn.keys()[i] for i in sample_train]\n",
      "    test_records_algn = [records_algn[i] for i in acc_test]\n",
      "    test_records_not_algn = [records_not_algn[i] for i in acc_test]\n",
      "    train_records_algn = [records_algn[i] for i in acc_train]\n",
      "    train_records_not_algn = [records_not_algn[i] for i in acc_train]\n",
      "    write_fasta(\"./data/aligned_train\", train_records_algn)\n",
      "    write_fasta(\"./data/unaligned_train\", train_records_not_algn)\n",
      "    write_fasta(\"./data/aligned_test\", test_records_algn)\n",
      "    write_fasta(\"./data/unaligned_test\", test_records_not_algn)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}